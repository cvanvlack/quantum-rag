\section{Challenges and Goals\label{sec:goals}}

\subsection{Selected Applications in the Theory Domain}
\label{subsect_Theory}

In this section, we will introduce a series of interesting theoretical challenges in different theoretical domains, including many-particle physics, different flavours of lattice gauge theories, and neutrino physics. The applications are dealing with relatively low dimensional systems, which however preserve some of the key aspects and criticality, which characterize the systems at the full scale. 
Clearly, the list of identified topics cannot be exhaustive. The choice is mainly motivated by the research interests of the co-authors of this paper. 
However, we hope that the solutions proposed for this selection of problems, and the corresponding algorithms, 
can be of inspiration in other domains not contemplated here.

Since most applications will deal with the dynamical aspects of the different model Hamiltonians, we will start this section with an introduction on methods for real-time simulations. 

\subsubsection{Simulations of Real-Time Phenomena}

Experimental results from high-energy physics labs, such as the Large Hadron Collider, come in the form of data on collision products. It is through scattering processes that we experimentally acquire a deep understanding of the fundamental physics, typically by reconstructing which composite quasiparticles are assembled during intermediate stages of the scattering event, and comparing their properties to the theoretical predictions from the standard model (and beyond).

It is clear, however, that this type of prediction presents several limitations. First of all, it is indirect, in the sense that the observed composite quasiparticle properties are compared, and not the scattering event distribution per se. Moreover, the analytic calculations of such quasiparticles are limited to those accessible via perturbation theory, in the form of Feynman diagrams, and thus no accurate predictions are expected for the quantum chromodynamics sector, which is far from perturbative. A substantial obstacle towards accurate model predictions of scattering phenomena is that Monte Carlo methods which excel at capturing equilibrium properties, are hindered when tackling out-of-equilibrium real-time dynamics, again, due to the sign problem and complex actions to numerically integrate.

From this perspective, gaining access to direct data of non-perturbative many-body real-time simulations of gauge theories would enable a complete paradigm shift. The simulation could immediately provide the statistics of products so that we could immediately compare them with the observed statistics of collected events from high-energy labs. Lattice gauge theories in the Hamiltonian formulation are perfectly suited for this task: while space dimensions are discretized (typically into a cubic lattice), time is kept as a continuous variable, and thus the many-body real-time evolution operator is formally well-defined for any arbitrary time interval. In this framework, the continuum limit can be safely approached without worrying about ultraviolet divergences.
Numerically computing such an evolution operator, and its action onto an arbitrary input state (for instance converging quasiparticle wavepackets) is however an exponentially hard problem in the lattice system size and requires the aid of quantum simulators or quantum-inspired numerical algorithms to be carried out in good approximation.

Both analog and digital quantum simulator strategies can be used to push towards this goal. In either case, the real-time evolution operator is applied to a set of qubits (or, more generally, \textit{qudits}) which encode the many-body quantum fields state. An analog quantum simulator approximates the target model Hamiltonian by implementing an instantaneous controllable Hamiltonian that is equivalent to the target at a chosen energy scale, and then lets the system evolve with time-independent controls~\cite{Banerjee2013}. This approach is inherently scalable, but it is limited by what types of interactions can be engineered. Conversely, digital quantum simulators aim at decomposing the action of the time evolution into a circuit of programmable quantum operations (for instance, gates)~\cite{Martinez2016}. This approach is more general, especially if the quantum resources form a universal set of gates, but it can be demanding in terms of scalability and coherence. Indeed, the number of qubits and the circuit depth required to perform such simulations are largely beyond the capabilities of current near term, noisy quantum 
devices~\cite{Lloyd96,Jordan2012}.

Alongside methods based on quantum hardware, we highlight the potential of Tensor Networks as a numerical strategy working on the same lattice Hamiltonian framework (discrete space, continuous time) as quantum simulators~\cite{Schollwock2011,Montangero2018a, Silvi2019a,Cirac2020}. \gls{tn} excel in describing lattice quantum states at equilibrium, even in multiple spatial dimensions, and even at finite densities~\cite{Dalmonte2016,Banuls2020TNreview,Felser2019,Magnifico2020}. Moreover, they can accurately capture out-of-equilibrium dynamics as long as the entanglement production is low (i.e.~the area laws of entanglement are not violated). While seemingly a strict requirement, it is actually a ubiquitous occurrence, from many-body localization, to slow quenches across phase transitions (Kibble-Zurek mechanism), to short-timescale transient phenomena under Lieb-Robinson bounds. Thus there are many physical systems whose dynamics are accurately captured by \gls{tn} (especially in one dimension).
Indeed, the first proof of principle demonstration of a scattering event in a lattice gauge theory in one-dimension was shown in~\cite{Pichler2016} where two-wave-packets collisions and subsequent time evolution of the created entanglement was studied. A more refined study of the process was presented in~\cite{Rigobello2021}. 

When specifically addressing scattering problems, with either classical or quantum simulations, there is an additional conceptual complexity which gets added to the already-serious problem of executing the many-body dynamics: namely, preparing the input state. Initial quantum states in particle colliders experiments typically involve localized wave packets of composite quasiparticles, for example hadrons. Written in the elementary quantum fields, these wave packets have a well defined center-of-mass momentum and overall number density (usually one quasiparticle), but their internal wave function can be very complex. Clearly, the scattering simulation must include strategies to build these states (and control their momentum) by carefully manipulating the elementary quantum fields encoded as qudits, starting from the (entangled) dressed vacuum.
Proposals to achieve such input-state preparation have been put forward for \gls{qtn}~\cite{VanDamme21,dborin2022} but the optimal general strategy is still unclear, and requires further investigation. Notice that this problem will remain when it becomes possible to study scattering processes in future quantum processors. Thus, any partial or final solution developed for tensor network will be highly valuable also for future quantum computations and the simulation of scattering processes. 
Let us mention in passing that other real-time phenomena, such as quenching, see e.g.~\cite{Banuls:2022iwk,Banuls2019b}, have also been studied with \gls{qtn} techniques. 

\subsubsection{(2+1)D QED}
\label{subsubsect_2+1QED}
As mentioned in the introduction, \gls{2p1D} \gls{qed} is one of the simplest quantum field theories that nevertheless retain interesting physics: for example it shares with \gls{qcd} important properties such as asymptotic freedom and confinement, and it is an excellent starting point for future analysis of more intricate theories.
We therefore propose \gls{2p1D} \gls{qed} as a very suitable benchmark and testbed model to explore the potential of quantum computing and, in particular, to compare it to \gls{tn} calculations.

The most used classical method to study lattice gauge theories numerically nowadays is the \gls{mcmc} approach, see the recent \textit{FLAG review}~\cite{FlavourLatticeAveragingGroupFLAG:2021npn}. While \gls{mcmc} can reach lattice sizes of order of $100^3\times 200$, which are currently unthinkable for \gls{qc} and \gls{tn} techniques, the Hamiltonian formulation used for the latter methods has several advantages.
For example, \gls{mcmc} suffers from very large autocorrelation times towards the continuum limit~\cite{Schaefer:2010hu}. In the regime of small to very small lattice spacing, we can take advantage of quantum computing or tensor network approaches that do not have this drawback. 
Furthermore, the Euclidean path integral used by \gls{mcmc} is afflicted by the infamous \textit{sign problem}~\cite{Troyer2005} which makes the study of quantum field theories at non-zero fermion densities impossible. More specifically for lattice \gls{qcd}, this prevents the exploration and characterization of regions of the phase diagram at non-zero baryon density, which are relevant to understand the early universe, neutron stars, or the transition to a quark-gluon plasma.  
Another important aspect is the limitation for classical \gls{mcmc} techniques in the presence of a topological term which, in stark contrast, can be treated  straightforwardly in the Hamiltonian formulation, i.e. with \gls{qc} or \gls{tn}. Finally, a Hamiltonian approach will enable the study of real-time phenomena such as scattering processes, thermalization or the dynamics of physical systems after quenching, see the discussion in Sec.~\ref{Introduction} and below.

Although we are fully aware of the advancements of \gls{tn}~\cite{Banuls2019SimulatingLG}, in the spirit of this paper, we will focus on the quantum computing approach to study quantum field theories and, in particular, on the example of \gls{2p1D} \gls{qed}.

 Another pillar of quantum information science and technology is analog quantum simulators~\cite{Hauke2013,Georgescu2014,Acin2018} which allow direct experimental access to various quantum many-body phenomena. Given recent advancements in quantum-simulator technology such as single-atom resolution through gas microscopes~\cite{Bakr2009,Bakr2010,Gross2021} and overall high levels of precision and control~\cite{Bloch2008}, quantum simulators have become an attractive venue on which to probe high-energy phenomena~\cite{Pasquans_review,Dalmonte2016,Zohar2015a,aidelsburger2021cold,zohar2021quantum}, affording the precious advantage of accessible temporal snapshots at any stage of the system dynamics. The \textit{modus operandi} of quantum simulators is to map a \textit{target model} described by a Hamiltonian $\hat{H}_0$ onto another quantum model amenable for realization in an experimental platform. This mapping is almost never exact but will lead to an effective model where $\hat{H}_0$ arises up to leading order in perturbation theory, along with (undesired) subleading terms $\lambda\hat{H}_1$, with strength $\lambda<1$. In the context of gauge theories, the model $\hat{H}_0$ hosts a gauge symmetry generated by local operators $\hat{G}_j$, while $\hat{H}_1$ explicitly breaks it.

Initially, quantum simulators of gauge theories were restricted to cold-atom realizations of building blocks for both $\mathbb{Z}_2$~\cite{Schweizer2019} and $\mathrm{U}(1)$ gauge groups~\cite{Mil2020}. The experiment of Ref.~\cite{Schweizer2019} employed two species of bosonic cold atoms in a double-well potential. Periodic driving resonant at the on-site interaction strength and with the appropriate fine-tuning of the modulation parameters resulted in an effective Floquet Hamiltonian with the desired $\mathbb{Z}_2$ gauge symmetry. On the other hand, the experiment of Ref.~\cite{Mil2020} employed inter-species spin-changing collisions to model the gauge-invariant coupling between matter and gauge fields. Although groundbreaking in their own right, these experiments were restricted to building blocks and suffered from uncontrolled subleading gauge-noninvariant processes that limited useful coherent times~\cite{Halimeh2020}.

To probe gauge-theory physics relevant to high-energy phenomena, it became essential to devise experimentally feasible methods that could enable large-scale implementations on quantum simulators. This was made possible through the introduction of \textit{linear gauge protection}. It could be shown that gauge violations were suppressed controllably up to all experimentally relevant timescales~\cite{Halimeh2021}. Such a term naturally arises in mappings of spin-$1/2$ representations of \gls{1p1D} lattice \gls{qed} on a tilted Bose--Hubbard superlattice, which has recently enabled the realization of a large-scale $\mathrm{U}(1)$ quantum link model on a quantum simulator composed of $71$ superlattice sites~\cite{Yang2020}. Stabilized gauge invariance was certified by adiabatically sweeping through Coleman's phase transition and observing a gauge violation of less than $10\%$ throughout the entire dynamics. This setup was then employed to study thermalization in the $\mathrm{U}(1)$ quantum link model~\cite{Zhou2022,Wang2022}, and further extended to probe rich quantum many-body scarring regimes in this gauge theory~\cite{Su2023}. Extensions of this large-scale platform with linear gauge protection have been proposed for higher spatial dimensions~\cite{Osborne2022} and for larger spin representations of the gauge field~\cite{osborne2023spins}.

In what follows and to be concrete, we consider the formulation of \gls{qed} on a two-dimensional space lattice with lattice spacing $a$.
Since the Hamiltonian formalism is to be considered for its eventual application on quantum devices, an encoding needs to be applied to represent the fermionic and gauge degrees of freedom, which cannot be fully eliminated in \gls{2p1D}.  To deal with the fermionic \textit{doubling problem}~\cite{NIELSEN1981219,PhysRevD.16.3031,rothe2012lattice}, i.e. the existence in $d$-dimensions of $2^d$ flavors (or tastes) for each physical particle, many different discretizations have been considered. One of the most used is the \gls{ks} formulation~\cite{Kogut1975}, which separates fermionic and antifermionic degrees of freedom and assigns them to alternate sites of the lattice. Therefore the fermions and antifermions are associated with a single component field operator $\hat{\phi}_{\vec{n}}$, with $\vec{n}=(n_x,n_y)$ as the coordinates of the lattice sites. The parity of the coordinate $n_x + n_y$ determines the type of matter associated to the site (i.e., with particles (antiparticles) placed on even (odd) sites).
The links of the lattice are identified by a site $\vec{n}$ and a direction $\mu = x,y$ emanating from that site. After introducing a proper discretisation of the U(1) group, such as $\mathbb{Z}_{2L+1}$ ($L \in \mathbb{N}$), the electric field operators for each link $\oper{E}_{\vec{n}, \mu}$ take integer eigenvalues $\oper{E}_{\vec{n}, {\mu}}\ket{e_{\vec{n}}}=e_{\vec{n}}\ket{e_{\vec{n}}}, e_{\vec{n}} \in \mathbb{Z}$. It is then necessary to truncate this number of eigenvalues to $(2l+1)$ ($l \in \mathbb{N}$ and $l\le L$), to represent the gauge fields on the (finite-size) quantum circuit. On the links, we also define the link operators
\begin{eqnarray}
\oper{U} = \mathrm{e}^{iag\oper{A}_{\mu}(\vec{n})},
\end{eqnarray}
where $\oper{A}_{\mu}(\vec{n})$ is the vector field and $g$ is the coupling constant. 
These operators obey the following commutator
\begin{equation}
[\oper{E}_{\vec{n}, \mu},\oper{U}_{\vec{n}^\prime, \nu}] = - \delta_{\vec{n},\vec{n}^\prime} \delta_{\mu,\nu}\oper{U}_{\vec{n}, \mu},
\end{equation}
and therefore act as a lowering operator on electric field eigenstates, namely $\oper{U}_{\vec{n}, {\mu}}\ket{e_{\vec{n}}}=\ket{e_{\vec{n}}-1}$. Physically $\oper{U}_{\vec{n},\mu}$ measures the phase proportional to the coupling acquired by a unit charge moved along a link. 

Setting the lattice spacing $a=1$, the Hamiltonian can thus be written as~\cite{PhysRevD.16.3031}:
\begin{equation}\label{eq:fullH}
\oper{H}_{tot} = \oper{H}_E + \oper{H}_B + \oper{H}_m + \oper{H}_{kin}.
\end{equation}

The first term is related to the electric interaction,
\begin{eqnarray}
	\oper{H}_E = \frac{g^{2}}{2} \sum_{\vec{n}}\left(\oper{E}^{2}_{\vec{n}, x} 
	+ \oper{E}^{2}_{\vec{n}, y}\right).
\end{eqnarray}

The second term in $\oper{H}_{tot}$ defines the magnetic interaction,
\begin{eqnarray}
\oper{H}_B = -\frac{1}{2g^{2}} \sum_{\vec{n}} \left(\oper{P}_{\vec{n}} + \oper{P}_{\vec{n}}^{\dag}
\right),
\end{eqnarray}
where $\oper{P}_{\vec{n}} =  \oper{U}_{\vec{n},x}\oper{U}_{\vec{n}+x,y}\oper{U}^{\dag}_{\vec{n}+y,x}\oper{U}^{\dag}_{\vec{n},y}$ is called plaquette operator.

The last two terms describe the fermionic part, i.e. the mass term 
\begin{eqnarray}
\oper{H}_{m} = m \sum_{\vec{n}} (-1)^{n_x+n_y} \oper{\phi}^\dag_{\vec{n}} \oper{\phi}_{\vec{n}},
\end{eqnarray}
with $m$ the fermion mass, and the kinetic term, corresponding to the creation or annihilation of a fermion-antifermion pair on neighbouring lattice sites,
\begin{eqnarray}
\oper{H}_{kin} = \sum_{\vec{n}} 
\frac{(-1)^{n_{xy}}}{2} (\oper{\phi}_{\vec{n}}^{\dag} \oper{U}_{\vec{n},
 x} \oper{\phi}_{\vec{n}+ x} + {H.c.}),
\end{eqnarray}
where  for the links in the $x$-direction $n_{xy}=1$ and for those in the $y$-direction $n_{xy}=(-1)^{n_{x}}$.

An alternative to the \gls{ks} formulation is the \textit{Wilson approach}~\cite{PhysRevD.10.2445,angelides2023computing}. It introduces a second-order derivative term in the Hamiltonian that vanishes linearly with the lattice spacing in the continuum limit.
The main advantage of this approach is that the number of qubits needed to represent the gauge fields is lower than the one utilised in the \gls{ks} approach, and therefore has a lower resource requirement~\cite{Mathis2020}.


One of the challenges of simulating the gauge theory with quantum computers is to find a resource efficient way to map all its degrees of freedom onto a quantum computer. This holds, in particular, for the bosonic gauge degrees of freedom. Here several Ans\"atze exist in the 
literature~\cite{Clemente2022a,Bauer:2021gek,Haase2021resourceefficient,Kane:2022ejm} and it is important to test these approaches against each other, evaluate their advantages and shortcomings, and identify the most resource efficient discretization and truncation scheme for their implementation on a quantum computer.  

Once we have developed the most suitable encoding, we need to choose the most appropriate simulation technique depending on our goal. For example, in order to compute the ground state energy (the low-lying spectrum) of our Hamiltonian we can apply \gls{vqe}~\cite{Peruzzo2014} (\gls{vqd}~\cite{Higgott2019} or \gls{ssvqe}~\cite{PhysRevResearch.1.033062}).
Other approaches could be imaginary time evolution~\cite{McArdle_2019} or creating a suitable operator basis~\cite{PhysRevResearch.2.043140}. 

\subsubsection{(2+1)D SU(2)}

With the long-term goal of quantum chromodynamics in mind, it is important to consider non-Abelian gauge theories. A Yang-Mills theory with SU(2) gauge symmetry group is a natural first step. The standard Kogut-Susskind Hamiltonian formulation of lattice gauge theories is defined as 

\begin{equation}
    \begin{aligned}
        \hat{H} &= \frac{1}{2a} \sum_{\vec{n}} \sum_{\alpha,\beta} \left(i \hat{\psi}_\alpha (\vec{n})^{\dag}\hat{U}_{\alpha\beta}(\vec{n},i)\hat{\psi}_\beta(\vec{n}+\hat{i})\right.\\
         &\phantom{ \frac{1}{2a} \sum_{\vec{n}} \sum_{\alpha,\beta}}\left.+ (-1)^{\vec{n}} \hat{\psi}_\alpha (\vec{n})^{\dag}\hat{U}_{\alpha\beta}(\vec{n},j)\hat{\psi}_\beta(\vec{n}+\hat{j})
        + \text{H.c.} \right) \\
        & + m\sum_{\vec{n}} \sum_{\alpha} (-1)^{\vec{n}} \hat{\psi}^{\dag}_{\alpha}(\vec{n})\hat{\psi}_{\alpha}(\vec{n})\\
        & + \frac{g^2}{2a^{d-2}} \sum_{\vec{n},l}\sum_{b} [\hat{E}^b(\vec{n},l)]^2 \\
        & -\frac{1}{2a^{4-d}g^2}\sum_{\vec{n}} \sum_{\alpha, \beta, \gamma, \delta}\left(\hat{U}_{\alpha \beta}(\vec{n},i)\hat{U}_{\beta \gamma}(\vec{n}+\hat{i},j)\right. \\
        & \phantom{-\frac{1}{2a^{4-d}g^2}\sum_{\vec{n}} \sum_{\alpha, \beta, \gamma, \delta}}\left.\times\hat{U}_{\delta \gamma}^{\dag}(\vec{n}+\hat{j},i)\hat{U}_{\alpha \delta}^{\dag}(\vec{n},j) + \text{H.c.}\right).
    \end{aligned}
    \label{KS_Ham}
\end{equation}
where $\vec n$ is a lattice site and $l$ is a direction on the spatial lattice. Greek indices such as $\alpha=1,2$ (and $\beta$, $\gamma$, $\delta$) are indices in the fundamental representation of the SU(2) group, whereas $b=1,2,3$ is an adjoint index. Physically, $\hat E^b$ is the chromoelectric field and, as discussed for \gls{qed}, the chromomagnetic field arises from the plaquette term that appears last in Eq.\eqref{KS_Ham}. The physical parameters in this Hamiltonian are the fermion mass $m$ and the gauge coupling $g$.

The problem of simulating lattice gauge theories on a universal quantum computer using qubits as the basic degrees of freedom was defined in general terms in~\cite{Byrnes2006}. It was shown in~\cite{kan2021lattice} that lattice gauge theories in any spatial dimension can be simulated on quantum hardware using a polynomial number of gates in the number of lattice sites, bosonic gauge field truncation, and simulation time.

Other approaches use quantum simulators to emulate the physics of non-abelian gauge theories. In these implementations, gauge invariance is a direct consequence of some underline symmetry of the quantum simulator. For instance, angular momentum conservation is used to realise the SU(2) Yang-Mills model~\cite{Zohar2013a} and nuclear spin conservation in alkaline-earth atoms is used to mimic SU(N) models within the quantum link formulation~\cite{Banerjee2013}. In this respect, the quantum link formulation appears as a natural formulation for the quantum simulation of the non-abelian model which was proposed within a Rydberg-based architecture~\cite{Tagliacozzo2013a} and within superconducting circuits~\cite{mezzacapo2015non}.

The first quantum simulation of a SU(2) lattice gauge theory on IBM superconducting hardware was done in~\cite{Klco2019}. Subsequently, exploratory computations were conducted for one-dimensional SU(2) on an IBM superconducting platform~\cite{Atas2021}. This implementation combined the fact that a one-dimensional theory with open boundary conditions allows one to rewrite all gauge field degrees of freedom as long-range interactions among fermions with \gls{vqe} to study both meson and baryon states. There have further been one-dimensional SU(3) quantum simulations~\cite{farrell2022preparations,atas2022real,farrell2022preparationsb}, and error mitigation methods have been applied to study the time evolution of non-abelian models~\cite{rahman2022self}. 

In contrast to the one-dimensional case, studies of two-dimensional SU(2) gauge theory require both fermion and gauge field degrees of freedom. Several formulations have been proposed~\cite{raychowdhury2020loop,davoudi2021search}, and practical studies of each will provide valuable information for understanding their advantages and disadvantages.

The choice of basis for the local degrees of freedom in the implementation of a non-abelian gauge model is important~\cite{davoudi2022general,tong2022provably}. Usually, one needs to study the effect of discretisation or truncation on the physical results of the models~\cite{hartung2022digitising,jakobs2023canonical}. One option to discretise non-abelian theories is to use finite-dimensional subgroups~\cite{gustafson2022primitive,zohar2017digital,bender2018digital}, which can be efficiently implemented within a Rydberg base architecture~\cite{gonzalez2022hardware,gonzalez2023fermionic,zache2023quantum,zache2023fermion}. Early computations of SU(2) gauge fields on quantum hardware have used lattices with up to 6 plaquettes in total~\cite{Klco2019,rahman20212}. An initial study was also carried out for SU(3) in~\cite{Ciavarella2021}.  

Upcoming computations can build upon the lessons learned from these first steps, and grow in scale and scope alongside the continuing progress in quantum hardware deployment in the noise intermediate scale quantum era.

\subsubsection{Quantum Link Models and D-Theory}

D-theory is an alternative formulation of lattice field theory in which continuous, classical fields are replaced by {\em discrete}, quantum degrees of freedom, which undergo {\em dimensional reduction} from an extra dimension of short extent \cite{brower2004d}. In the D-theory approach, lattice gauge theories are realized via quantum link models
\cite{horn1981finite,orland1990lattice,chandrasekharan1997quantum,brower1999qcd,wiese2013ultracold,wiese2014towards}. Quantum links are generalized quantum spins endowed with an exact gauge symmetry, which is located on the links of a spatial lattice.

Quantum links reside in finite-dimensional irreducible representations of an embedding algebra. This is in contrast to the standard Wilson-type lattice gauge theory, which is based on an infinite-dimensional representation on each link. Quantum links with a $U(N)$ or $SU(N)$ gauge group reside in the embedding algebra $SU(2N)$. In particular, $U(1)$ quantum link models are formulated with ordinary $SU(2)$ quantum spins. $SO(N)$ and $Sp(N)$ quantum link models are realized with an $SO(2N)$ and $Sp(2N)$ embedding algebra, respectively. Since $SU(2) = Sp(1)$, an $SU(2)$ quantum link model can be realized with a simple $Sp(2) = SO(5)$ embedding algebra.
 
The simplest Abelian $U(1)$ quantum link model is realized with ordinary quantum spins $1/2$, which can be embodied by individual qubits. These dynamics have already been represented by quantum circuits in a resource-efficient manner \cite{wiese2022quantum}. The implementation of the $U(1)$ quantum link model on a triangular lattice is particularly simple, because it takes advantage of the heavy hexagonal lattice topology underlying the 127-qubit IBM Eagle chip \cite{banerjee2022nematic}. 

The simplest non-Abelian $SU(2)$ quantum link model uses the embedding algebra $Sp(2) = SO(5)$, which has a 4-dimensional fundamental representation that can be embodied by a pair of qubits residing on each lattice link. By an exact duality transformation, this $SU(2)$ quantum link model can be expressed in terms of $Z(2)$-valued height variables, which can even be embodied by individual qubits \cite{banerjee2018s}. This model is also interesting from a condensed matter perspective, because it is closely related to the quantum dimer model on the Kagom\'e lattice which has a rich, non-trivial phase structure. It would be very interesting to construct a quantum circuit, similar to the one for the $U(1)$ quantum link model on the triangular lattice, in order to perform quantum computations of the real-time dynamics of $SU(2)$ gauge theories.

\subsubsection{(1+1)D $\CP(N-1)$ Models from $(2+1)$D $SU(N)$ Quantum Spin Ladders}

\gls{1p1D} $\CP(N-1)$ quantum field theories are toy models that share many important features with \gls{3p1D} \gls{qcd}: they are asymptotically free,  have a non-perturbatively generated massgap, as well as $\theta$-vacua \cite{d19781n,eichenherr1978n}
In addition, they have non-trivial phase structure at non-zero chemical potential, including Bose-Einstein condensates with and without ferromagnetism \cite{evans20183}. 

The standard lattice formulation of $\CP(N-1)$ models at non-zero vacuum angle or at non-zero chemical potential suffers from similar sign and complex action problems as \gls{qcd} itself.
D-theory offers an alternative approach to standard lattice field theory, which uses {\em discrete} quantum (rather than continuous classical) degrees of freedom without compromising exact continuous symmetries including gauge symmetry. In asymptotically free theories (including \gls{1p1D} 
$\CP(N-1)$ models and \gls{3p1D} \gls{qcd}, the continuum limit is reached naturally (i.e.\ without any fine-tuning) via {\em dimensional reduction} from a higher-dimensional space-time, with a short extent of the extra dimension.
Interestingly, the finite-density and $\theta$-vacuum sign problems of the standard formulation have already been overcome by the alternative D-theory formulation, in which $\CP(N-1)$ models are regularized using $SU(N)$ quantum spin degrees of freedom \cite{beard2005study}
This formulation is also amenable to analog quantum simulations with ultra-cold alkaline-earth atoms in  optical lattices, which holds the promise to facilitate real-time simulations of their dynamics \cite{laflamme2016cp}
$\CP(N-1)$ models in the D-theory formulation are ideally suited as a testing ground for quantum computation, because, on the one hand, at least in some cases, advanced classical computational techniques are available  for validation, and, on the other hand, similar methods can be developed for lattice gauge theories, ultimately aiming at \gls{qcd}, in particular in the quantum
link formulation. 
The strategy behind D-theory, namely to formulate quantum field theory directly in terms of quantum degrees of freedom, is ideally suited for both quantum simulation and quantum computation.

The standard formulation of $\CP(N-1)$ models uses classical, Hermitean, idempotent $N \times N$-matrix fields $P(x)$
\[P(x)^\dagger = P(x) \ , \quad P(x)^2 = P(x) \ , \quad \mbox{Tr} P(x) = 1 \ , \]
with the Euclidean action
\[S[P] = \int d^2 x  \frac{1}{g^2} \mbox{Tr}\left[\p_\mu P \p_\mu P\right] + i \theta Q[P] \ ,\]
and the integer-valued topological charge
\[Q[P] = \frac{1}{\pi i} \int d^2 x \, \epsilon_{\mu\nu} \mbox{Tr} \left[P \p_\mu P \p_\nu P\right] \in \Pi_2[\CP(N-1)] = \Z \ .\]
The model is invariant under a global $SU(N)$ symmetry, $P(x)' = \Omega P(x) \Omega^\dagger$, $\Omega \in SU(N)$. 

The alternative D-theory formulation replaces the classical field $P(x)$ by $SU(N)$ quantum spins $T^a_x$ ($a \in \{1,2,\dots,N^2-1\}$) that obey the commutation relation
\[[T_x^a,T_{x'}^b] = i \delta_{xx'} f_{abc} T_x^c \ ,\] and reside on a 2-d spatial square lattice (of spacing $a$) with a long  $x_1$-direction (of extent $L$ with periodic boundary conditions) and a short $x_2$-direction (of extent $L'$ with open boundary conditions).
The even-parity sites $x \in A$ (with even $x_1 + x_2$) carry the fundamental representation 
$\{N\}$, $T_x^a = \lambda^a/2$ (where the $\lambda^a$ are Gell-Mann matrices), while the odd-parity sites $y \in B$ carry the anti-fundamental representation $\{\overline{N}\}$, $\overline{T}^a_y = - {\lambda^a}^*/2$. 
An antiferromagnetic $SU(N)$ quantum spin ladder (with $J > 0$) is then described by the 
nearest-neighbor Hamiltonian
\[H = J \sum_{\langle xy \rangle} T^a_x \overline{T}^a_y \ ,\] which commutes with the total $SU(N)$ spin $T^a = \sum_{x \in A} T^a_x + \sum_{y \in B} \overline{T}^a_y$.
In the presence of chemical potentials $\mu_a$ at inverse temperature $\beta$
the grand canonical partition function then takes the form
\[Z = \mbox{Tr} \exp(- \beta (H - \mu_a T^a)) \ .\]

Remarkably, this antiferromagnetic quantum spin ladder is a proper regularization for the \gls{1p1D} $\CP(N-1)$ quantum field theory. An even extent $L'/a$ of the short dimension corresponds to vacuum angle $\theta = 0$, while an odd extent implies $\theta = \pi$. For $L = L' = \beta = \infty$, the quantum antiferromagnet breaks the global $SU(N)$ symmetry down to $U(N-1)$ (at least for $N \leq 4$). 
This gives rise to dynamically generated, effective Goldstone boson fields $P(x)$ that reside in the coset space $SU(N)/U(N-1) = \CP(N-1)$.
Once $L'$ is made finite, the Mermin-Wagner theorem implies that $SU(N)$ can no longer break spontaneously. As a result, the previously massless Goldstone bosons pick up an exponentially small mass proportional to $\exp\left(- 4 \pi L' \rho_s/c N\right)$, where $\rho_s$ is the spin stiffness and $c$ is the spinwave velocity. 
For moderately large $L'/a \gtrsim 4$, the corresponding correlation length $\exp\left(4 \pi L' \rho_s/c N\right) \gg L'$ exceeds the extent of the short dimension and the system dimensionally reduces to the \gls{1p1D} $\CP(N-1)$ model. 
These dynamics, which may seem complicated at first glance, have been verified in great detail in quantum Monte Carlo simulations using classical computers. Already at the level of classical computation, the use of discrete quantum, rather than continuous classical, fundamental degrees of freedom has led to numerous algorithmic advantages, which facilitated efficient numerical simulations of $\theta$-vacua and
dense matter systems \cite{beard2005study,evans20183}

Analog quantum simulators for the $SU(N)$ quantum antiferromagnet have already been designed, using ultra-cold alkaline-earth atoms in an optical lattice, and are ready to be realized in the laboratory already today
\cite{laflamme2016cp}. 
This holds the promise to address the real-time dynamics, which remains inaccessible to classical simulation techniques. This would be the first time that an asymptotically free quantum field theory is studied with quantum simulation.
The simple nature of the quantum spin degrees of freedom and the ultra-local form of their Hamiltonian strongly suggest to also explore $\CP(N-1)$ models using digital quantum computation. 
In particular, in D-theory the $\CP(1)$ model with a global $SU(2)$ symmetry is regularized with ordinary $SU(2)$ quantum spins which can be embodied directly by individual qubits. 
Similarly, the $SU(3)$ quantum spins in the D-theory formulation of the $\CP(2)$ model are nothing but qutrits. 
The corresponding Hamiltonian dynamics can be realized with sequences of single-qubit and two-qubit (or single-qutrit and two-qutrit) quantum gates.
It is possible --- and already quite interesting --- to work with quantum spin chains (i.e.\ with $L'/a = 1$) rather than with quantum spin ladders ($L'/a > 1$).
In particular, for $L'/a = 1$ the antiferromagnetic  $SU(2)$ quantum spin chain corresponds to the \gls{1p1D} Wess-Zumino-Novikov-Witten conformal quantum  field theory in the continuum limit.
The corresponding $SU(3)$ quantum spin system, although it is not in the continuum limit, describes a strongly coupled $\CP(2)$ model at a first-order phase transition with spontaneously broken 
charge conjugation symmetry. 
This would allow, for example, real-time studies of false vacuum decay.


\subsubsection{Collective Neutrino Oscillations}
 
Neutrinos play a central role in extreme astrophysical events like core-collapse supernovae and neutron star binary-mergers as they dominate the transport of energy, entropy and lepton number. Due to the fact that neutrinos have masses and that 
the mass basis, denoted by $\{ | \nu_i \rangle\}_{i=1,3}$, is different from the flavor basis, neutrinos will experience oscillations in the population of the different flavors  components $(\nu_e, \nu_\mu , \nu_\tau)$. 

Given the importance of charge-current reactions, a detailed understanding of flavor oscillations in these settings is critical to predict their dynamical evolution. Given the high density of neutrinos in these environments, flavor oscillations are strongly affected by two-body neutrino-neutrino interactions, which render the neutrino cloud a strongly coupled many-body system. Direct solution of the evolution equations for general initial conditions can be exponentially hard with classical simulations, and the conventional approach is to rely on mean-field approximations~\cite{Duan2006,Duan2010,Tamborra2021}, which, however, do not include direct scattering between neutrino. Efforts in going beyond mean-field with classical computers were recently reviewed in~\cite{Patwardhan2023}. 

The complexity of neutrino physics 
persists even with the simplifying assumption that only two flavors (the electron flavor $\nu_e$ and one heavy flavor $\nu_x$) participate in the oscillation. With this assumption, one can model each neutrino as a set of interacting two-level systems and obtain the following Hamiltonian~\cite{Pehlivan2011}
\begin{equation}
\label{eq:ham_neutrinos}
H=\sum_{i=1}^N \boldsymbol{b}_i \cdot \boldsymbol{\sigma}_i + \lambda_e \sum_{i=1}^N \sigma^z_i + \frac{\mu}{2N}\sum_{i<j}^N \left(1-\cos(\theta_{ij})\right) \boldsymbol{\sigma}_i \cdot \boldsymbol{\sigma}_j\;, 
\end{equation}
where $\boldsymbol{\sigma}_i=(\sigma_i^x,\sigma_i^y,\sigma_i^z)$ is a vector of Pauli matrices acting on the i-th neutrino.
The first term in Eq.~\eqref{eq:ham_neutrinos} describes vacuum oscillations around the mass basis with $\boldsymbol{b}_i = \frac{\delta m^2}{4 E_i}( \sin(2 \theta_{\nu}), 0, - \cos(2\theta_{\nu}))$,
where $\delta m^2 = m_2^2 - m_1^2 $ is the square mass difference between mass eigenstates, $\theta_\nu$ is the mixing angle and $E_i$ the energy of the $i$-th neutrino. The second term in Eq.~\eqref{eq:ham_neutrinos} is generated by charge-current scattering with a background of electrons with coupling constant $\lambda_e=\sqrt{2}G_Fn_e$ with $G_F$ the Fermi constant and $n_e$ the electron density. This is the term responsible for the \gls{msw} effect due to the interaction of electrons with neutrinos experienced by neutrinos travelling in dense matter. Finally, the third term in Eq.~\eqref{eq:ham_neutrinos} is the neutrino-neutrino interaction generated by neutral-current weak reactions. Its coupling constant $\mu=\sqrt{2}G_Fn_\nu$ is directly proportional to the local neutrino density $n_\nu$, while the angular factor inside the sum encodes the spatial geometry of the problem through its dependence on the relative angle of propagation $\cos(\theta_{ij})=\boldsymbol{p}_i\cdot\boldsymbol{p}_j/(\|\boldsymbol{p}_i\|\|\boldsymbol{p}_j\|)$ 
where $\boldsymbol{p}_i$ is the momentum of $i$-th neutrino. This term prevents collinear neutrinos from interacting.

The Hamiltonian in Eq.~\eqref{eq:ham_neutrinos} can be used to describe the flavor evolution of a homogeneous gas of neutrinos at fixed density. Most neutrinos are however leaving the explosion region of the emitter (neutron stars, ...) where they have been generated and thus experience different local conditions as they move out. This can be incorporated by allowing the coupling constants $\lambda_e$ and $\mu$ to change with the distance $r$ from their emission, or equivalently with the time $t$ since they left the neutrino sphere (neutrinos are considered as ultra-relativistic particles moving at approximately the speed of light). With this, we are left with describing the non-equilibrium evolution of a large number of fermions interacting through an eventually non-adiabatic two-body Hamiltonian. Several extensions are possible to account e.g. for the full three-flavor structure~\cite{Siwach2022} or the presence of inhomogeneities~\cite{Stirner_2018} but are likely beyond the scope of the $100 \otimes 100$ IBM challenge. 

Current efforts to study the full many-body flavor dynamics generated by the Hamiltonian in Eq.~\eqref{eq:ham_neutrinos} beyond the mean-field approximation have been carried out under a number of additional simplifying assumptions. A popular one is to consider an average interaction strength, effectively removing the angular dependence in the two-body interaction turning it into a term proportional to the square of the total angular moment. This has the effect that the system becomes integrable using the Bethe-Ansatz~\cite{Pehlivan2011} and classical simulations have been performed in the past exploiting directly this property up to $N=9$~\cite{Cervia2019}. Using more direct integration approaches allowed $N=16$ to be reached~\cite{Patwardhan2021} while using \gls{mps} together with the Time Dependent Variational Principle systems up to $N=20$ were studied while keeping a good convergence with the bond dimension~\cite{Cervia2022}. Note that the latter simulations employed around $10^5$ time steps for the entire calculation and this leaves a direct comparison possibly out of range of the $100 \otimes 100$ IBM challenge.

Another common assumption is to neglect the MSW term proportional to the electron density using the argument that this coupling constant greatly dominates in the interior regions where many-body effects are expected to be important. The MSW term can be eliminated using a rotating wave approximation which ultimately produces a lower effective mixing angle. In general this is not necessary as this one-body term can be trivially fast-forwarded and included correctly, and efficiently, in the simulation by resorting to interaction picture schemes like the one proposed in~\cite{Rajput2022hybridizedmethods}. In the absence of this term, the Hamiltonian enjoys a global $U(1)$ symmetry generated by rotations around the mass basis which can be used to reduce the implementation cost. This strategy was used in~\cite{Yeter2022}, together with the use of IBM Qiskit’s isometry function to implement evolution in each sub-block, to study flavor oscillations up to  $N=4$ neutrinos systems. The approach has the advantage that the circuit depth does not increase as a function of the time steps but for large system sizes it would require an exponentially large number of gates. Part of the difficulty in including the two-body interaction is its all-to-all nature which naively does not fit well on devices with reduced connectivity. The problem can however be circumvented using an appropriate SWAP network scheme producing a circuit with $N$ layers of $~N/2$ nearest neighbor two-qubit gates each. This approach was proposed in~\cite{Hal21} where a $N=4$ neutrino simulations with a single Trotter step was carried out on IBM devices and has been shown to be advantageous to allow for classical simulations using \gls{mps}~\cite{Roggero2021}. Platforms that allow for all-to-all connectivity,  like trapped-ions, allow more flexibility but require a similar number of two-qubit operations. Due to their current higher fidelity, simulations have been reported for up to $10$ time steps with $N=4$ and for one time step up to $N=12$ neutrinos~\cite{Amitrano2022,Illa2022b}. Approaches using quantum annealers have also been proposed and applied for systems up to $N=4$~\cite{Illa2022a}.

The simplified neutrino oscillation problem described by Eq.~\eqref{eq:ham_neutrinos} is encoded quite naturally on a digital quantum computers with one qubit per neutrino. Current attempts to describe neutrinos on these platforms are still restricted to small $N$ values with rather simple initial conditions, usually wave-functions describing non-correlated neutrinos. Besides the description of larger neutrino number, challenges for future applications include the extension to more realistic initial conditions like initially thermalized neutrinos, or the evolution of these correlated systems over longer time to extract for instance asymptotic entanglement between neutrinos or characterize the relaxation dynamics to thermal states~\cite{Martin2023}. Time-evolution requires efficient algorithms to simulate the dynamics (see section~\ref{subsubsec_algo_dyn}). A first order \gls{pf} step for $N$ neutrinos costs $3N(N-1)/2$ \gls{cnot} operations while a second order step will cost $3(N^2-3N/2+1)$ \gls{cnot} gates~\cite{Amitrano2022}. The depths are instead $3N$ and $6N-3$, respectively. The implementation can be performed in a more hardware efficient way employing cross-resonance gates instead at the price of increasing the decomposition error. 
Furthermore, a hardware friendly approach to multi-product formulas can further reduce circuit depth and increase simulation accuracy~\cite{Vazquez2022}.
An additional possibility worth pursuing in the short term is the use of approaches based on \gls{vte} which allow for a circuit depth independent on the evolution time.

\subsection{Selected Applications for Experiments}
\label{subsect_Experiments}

High-energy physics experiments are characterized by the need to process a large amount of complex, highly structured data. Historically, large collaborations have relied on massively parallel computing infrastructure and pioneered the field of distributed computing with the LHC Computing Grid.
The need to search for processes with small production cross section together with next generation detectors,  generate a sheer size of the data sets to analyze, that require a new computing model, more efficient algorithms -- including data-driven techniques such as artificial intelligence--, and the integration of new hardware beyond the von Neumann architectures.
It is in this context that investigations about the introduction of quantum computing in \gls{hep} experiments is framed: the community is looking into accelerating or improving the different steps of data analysis and data processing chains.
Currently most of the work is focused on the development and optimisation of \gls{qml} algorithms implemented either as quantum neural networks (variational algorithms) or kernel methods~\cite {guan2021quantum, delgado2022quantum}. See Appendix~\ref{app:algos_limits} for a summary of these methods. The next section will give an overview of the range of algorithms under study as applied to HEP and their present limitations.

It is important to notice, however, that evaluating the performance of \gls{qml} algorithms on \gls{hep} data requires care: realistic applications have requirements that can not be easily accommodated on quantum devices, today.
The most critical issue is related to the size of data samples, together with their complexity.
Indeed, studies on the introduction of quantum algorithms (and \gls{qml} in particular) need to take into account both the total number of events that need analyzing (that can easily reach hundreds of thousands) and the large number of input features in each single event (typically in the order of tens or hundreds). The preferred approach today is hybrid:  a classical feature extraction and/or dimensionality reduction step is used to bring the classical input to a size that can be realistically embedded on 
%\gls{nisq} 
noisy, near-term quantum hardware. 
Depending on the complexity of both the dataset and the task,  different methods are used, ranging from linear PCA, to non-linear  trainable embedding or compression methods (auto encoders or other AI-based techniques)~\cite{albertsson2019machine}. The advantage of the latter is clearly their versatility and the possibility to train them together with the quantum algorithms for the specific task at hand.

In particular, trainable techniques allow an end-to-end optimisation of the reduced data representation (often referred to as “latent representation”), their embedding in quantum states and the quantum algorithm itself. A binary classification problem, such as the separation of  signal versus uninteresting background, is a common example: simultaneously training an autoencoder for data compression together with the corresponding classifier, ensures that the resulting latent representation exhibits maximal separation between the two classes.
Multiple examples have already proven the advantage of this approach in both the classical and quantum domain~\cite{collins2022machinelearning, Farina_2020, Cheng_2023, QADCERN, wozniak2023quantum,belis2021higgs}
In addition, a critical part of the quantum algorithm design and optimisation process is  aimed at reducing the number of input features needed by the quantum algorithm in order to perform its task, together with the definition of a minimal training set, that still ensures convergence and generalization capabilities. 

Finally, the compressed classical data is embedded, or loaded, onto quantum states for processing by the \gls{qml} algorithm. This step is commonly referred to as the state preparation step.  Different  techniques have been studied~\cite{lloyd2020quantum} that compromise between an optimal use of qubit states, exploiting in full the potential exponential advantage, and the need to efficiently map state preparation circuits on %\gls{nisq} We don't use this acronym, please
noise devices. In general, the choice of the data embedding strategy has an effect both on the performance of the overall algorithm and on its interpretation (as, for example, in the kernel formalism) as mentioned in Sec.~\ref{sec:algs}.

Taken all together, these  steps  have made possible the design and implementation of quantum algorithms for most of the tasks in the typical data processing chain, albeit at a reduced scale. Access to the $100 \otimes 100$ quantum hardware, combined to data reduction techniques is likely to bring current prototypes to a much more realistic size.

\subsubsection{Rare Signal Extraction}
Extracting rare signals from background events is an essential part of data analysis in the search for new phenomena in \gls{hep} experiments. In this section we will cover algorithms, methods and limitations of this area of research, giving some references which, for sure, do not represent a complete picture of the state of art.

Posed as a classification task, rare signal extraction faces an imbalance problem in the number of samples belonging to the signal class versus the number of samples from the background class.
Entry level cases are the ones where a single feature is powerful enough to discriminate the process of interest while more complicated cases rely on multi-variate analysis of many features to get to a reasonable level of discrimination power.

In the machine learning community, techniques for learning from imbalanced data are well established, and for the \gls{hep} case, analysis methods developed in~\cite{Britsch:2008mxb, extr_imb} have been effectively implemented. An alternative approach to classification with imbalance techniques is anomaly detection~\cite{Edelen:2021vcs,anoma2}.
In the following we touch upon some modern class imbalance techniques adopted in the community, focusing on novel loss functions and data re-sampling techniques.
However, the main goal here is not merely the classification task but also the generation of predictions with their corresponding uncertainties. In particle physics, as in other scientific domains, if uncertainties are not presented the picture is almost incomplete.

Using the accuracy of a classifier as a metric for rare events can be misleading as it says nothing about the signal, in terms of distribution and feature importance. The ROC curve is a good general purpose metric, providing information about the true and false positive rates across a range of thresholds, and the area under the ROC curve (AUC) is a good general purpose single number metric.
Nevertheless, when dealing with imbalanced data the precision-recall curve is the preferred metric, where the recall represents a measure of how many true signal events have actually been identified as signal and precision quantifies how likely an event is to truly be signal and depends on how rare the signal is. Different strategies can be used like under-sampling the majority class or oversampling the minority class, where the former is preferred because of the potential overfitting resulting from oversampling~\cite{Nguyen2009BorderlineOF}. Moreover, the standard algorithm can be modified by playing with the hyperparameters of the loss and by adding an additional penalty for misclassification. For instance, following~\cite{focalLF}, a modified version of the cross entropy loss function used for binary classification to differentiate between easy- and hard-to-classify samples is the focal loss function: 
\begin{equation}
FL = -(1-p_t)^\gamma \log(p_t) \, .
\end{equation}
Here $p_t$ is the model’s estimated probability that a given event belongs to the signal class and $\gamma$ is the modulating parameter. As $\gamma$ is increased the rate at which easy-to-classify samples are down weighted also increases.
As pointed out previously, not only should the classification be efficient but also the related prediction uncertainties.
Current approaches for this include dropout training in Deep Neural Networks as approximate Bayesian inference, variance estimation across an ensemble of trained deep neural networks, and Probabilistic Random Forest~\cite{modeluncertainty}.
For example, such techniques have been used in for the measurement of the longitudinal polarization fraction in same-sign $WW$ scattering~\cite{Ballestrero_2018} and for the decay of the Higgs boson to charm-quark pairs~\cite{Aaboud_2018}.

Same-sign $WW$ production at the LHC is the \gls{vbs} process with the largest ratio of electroweak-to-\gls{qcd} production. As such it provides a great opportunity to study whether the discovered Higgs boson leads to unitary longitudinal \gls{vbs}, and to search for physics \gls{bsm}.
Confirming or refuting the unitarity of \gls{vbs} requires not just a measurement of $pp\to jjW\pm W\pm$, but of the fraction of these events where both $Ws$ are longitudinally polarized (LL fraction). The fraction of longitudinally polarized events is predicted to be only a fraction $ 0.07$ to the total number of events in the \gls{sm} at large dijet invariant mass $(m_{jj})$~\cite{Ballestrero_2018} making this a challenging measurement.
Common techniques for this kind of use cases are Random Forest with imbalanced implementation, Gradient Boosted Decision Tree and Deep Learning models with standard or focal loss function. Overall, all of the machine learning models significantly outperform the kinematic variables approach~\cite{Grossi_2020}. 

The second application of class imbalance techniques is the measurement of Higgs boson decays to charm-quark pairs. Searches for the decay of the Higgs boson to charm-quarks have produced only weak limits to date.
Again, one of the reasons for this poor performance is the \gls{sm} the rate for $h \to b\bar{b}$ is about 20 times larger than the rate for $h \to c\bar{c}$. 
The standard approach relies on tagging the flavour of the jets, which involves discriminating charm initiated jets from bottom jets, or vice versa.
The primary technique used currently in this case is Boosted Decision Trees, mainly structured as binary classification problem, where the community effort is devoted to the definition of ad-hoc flavour tagging through the use of the class imbalance techniques instead of general purpose ones~\cite{Aaboud_2018}.

It is natural to ask whether quantum computing algorithms could be used to support these complicated tasks. However, it is not evident where a quantum algorithm could provide a systematic advantage with respect to these classical approaches. Possible directions of research should answer the following questions: can we overcome the problem of lack of density or insufficiency of information for these problems? Can we better explore and analyse the feature space that describes those problems? Could \gls{qml} methods, which employ quantum models to encode input data into a high-dimensional Hilbert space and extract physical properties of interest from the quantum state, be an alternative approach to signal detection? A particularly intriguing direction for quantum approaches here could be the possibility of training directly on experimental data~\cite{Metodiev_2017} that can be directly analysed as quantum data.

Overall, in the absence of a clear hint of new physics in \gls{hep} experiments, a data-driven, model-agnostic search for rare signals has gained considerable interest. Anomaly detection, realized using unsupervised machine learning, is the most commonly used technique and will continuously becoming important in \gls{hep} analysis workflow. 
The feasibility of anomaly detection is investigated in~\cite{PhysRevD.105.095004} with \gls{vqa}-based \gls{qae}. With the benchmark process of $pp \to H \to t\bar{t}$ for signal, the \gls{qae} performance for anomaly detection has been compared with that from a classical autoencoder, showing a faster convergence in the quantum case. Recently, in~\cite{schuhmacher2023unravelling}
 the authors find that employing a \gls{qsvc} trained to identify the artificial anomalies, it is possible to identify realistic \gls{bsm} events with high accuracy. In parallel, they also explore the potential of quantum algorithms for improving the classification accuracy and provide plausible conditions for the best exploitation of this novel computational paradigm. Additionally, in~\cite{QADCERN}  the authors found evidence that quantum anomaly detection using a \gls{qsvm} could outperform the best classical counterpart.  In~\cite{bermot2023quantum} an \gls{aqgan} is introduced to identify anomalous events (\gls{bsm} particles). Interestingly, this model can achieve the same anomaly detection accuracy as its classical counterpart using ten times fewer training data points. 


Overall, current quantum-classical hybrid \gls{qml} for rare signal extraction are  largely based on two algorithms: \gls{vqa}~\cite{VQA2021} and \gls{qsvm} with kernel method~\cite{havlivcek2019supervised,Schuld2019QML}.
The quantum kernel-based \gls{qsvm} has a potential for good trainability due to a convex cost-function landscape, and this property could be beneficial for the $100 \otimes 100$ challenge. It is however pointed out that the kernel function would exponentially concentrate to a fixed value with the number of qubits unless the $U({\boldsymbol x})$ is properly designed~\cite{thanasilp2022exponential}, analogously to the barren plateau in \gls{vqa}. 

\gls{vqa}-based \gls{qml} methods are generally known to be affected by the infamous barren plateau problem, where a non-convex landscape of cost function causes the gradients to vanish exponentially in the number of qubits,
as detailed in Sec.~\ref{subsec:qml_limitations}. 
With the $100 \otimes 100$ IBM challenge, overcoming barren plateaus may be critical for \gls{qml} applications to signal extraction. The approach based on so-called geometrical \gls{gqml}, that exploits prior knowledge to the problem, such as symmetry presented in the data at hand, will be promising for applications to \gls{hep} data analysis. However, experimental data are the result of a complex convoluted effect given by different layers of interaction, from parton shower to detector effects. This would eventually destroy any desirable symmetry of the data. Alternatively, quantum models in an overparameterized regime may have a desirable cost landscapes. This motivates exploring \gls{gqml} models and/or overparameterization in a realistic \gls{hep} data analysis flow. We should also pursue how efficiently a \gls{qml} model can generalize to unseen test data with fewer trainable parameters or less training data, and also consider the possibility of re-using well known techniques from classic machine learning, like ensemble, where, for instance, the effect of noise could be mediated by the structure of the algorithm~\cite{incudini2023resource}.


\subsubsection{Pattern Recognition Tasks: Reconstructing Particle Trajectories and Particle Jets}
Multiple steps in the experiments data processing chains can be collected into the general category of pattern recognition or the problem, given a certain number of measurements of an object (such as the raw energy measured by the sensors in a detector, or its spatial coordinates), of associating them to a specific instance: for example a particle trajectory, a particle type, the particle jet that originates from the hadronization of a specific parton (jet).   
In \gls{hep}, this problem has high dimensionality, since the detector sensors are arranged in highly granular structures, the objects represent physics properties and the object classes are typically exclusives (an energy deposition belongs to one and only one trajectory). 
Two examples are indeed represented by the reconstruction of charged particles and the reconstruction of jets, together with the identification of their properties.

The reconstruction of charged particle trajectories, \textit{tracking}, is an essential ingredient in event reconstruction for \gls{hep}. Particle track candidates are built from space points corresponding to energy deposits left by charged particles - or \textit{hits} - as they traverse the sensitive detector material. The track parameters (e.g. position and curvature) hereafter computed are used in subsequent processing steps throughout the reconstruction and analysis of data to compute physics observables.

In collider particle physics, a jet is a collection of stable particles collimated into a
roughly cone-shaped region. Jets arise from the fragmentation of quarks and gluons
produced in high-energy collisions. During the collision, the \gls{qcd} confinement the
quarks and gluons are subjected to is broken, yielding a spray of color-neutral
particles that can be experimentally measured in particle detectors. Jets have played
and are playing a fundamental role in collider physics. Events with three jets in $e^+ e^-$
collisions demonstrated the existence of the gluon. Nowadays jets produced by the
fragmentation of heavy quarks, namely $b$ and $c$ quarks are crucial for several studies
in particular to determine the Higgs boson couplings. In the latest years tools have
been developed to disentangle different kinds of jets.

\paragraph{Track Reconstruction}
Several current and future \gls{hep} experiments will explore high intensity scenarios going to extreme regimes with thousand of charged particles crossing a square centimeter of sensitive detector. Furthermore, depending on the process under study and the detector layout, each track can consist of a variable number of measurements. The multiplicity of possible track candidates from the input space-points scales quadratically or cubically with the number of hits. Therefore tight selections on the input space-points are required in order to narrow down the search space. Nevertheless, track reconstruction is one of the largest users of CPU time in \gls{hep} experiments, strongly motivating the R\&D of novel approaches.

Several approaches have been proposed to address the tracking problem and can be roughly divided into global and local approaches. Global tracking methods approach track reconstruction as a clustering problem, thus considering all the space-points at once, whereas local tracking methods generally consist of a series of steps executed sequentially. Several studies have been performed for both global~\cite{Funcke:2022dws,Crippa:2023ieq} and the local~\cite{Magano:2021jzd} methods, finding a potential reduction of computational complexity for the latter. 

First proposals to solve the particle track reconstruction problem on a quantum computer focused on converting the problem to a \gls{qubo} problem~\cite{bapst_pattern_2019, zlokapa2021}. This way, one can group two (doublets) or three (triplets) hits from consecutive detector layers and binary values represent if a given doublet or triplet corresponds to a particle track. 
There have been several proposals in the literature on how to determine the coefficients of the \gls{qubo} either based on geometry or impact on the overall energy of the \gls{qubo}~\cite{Funcke:2022dws, schwaegerl2023particle, Crippa:2023ieq}. In its most general form, one can write such a \gls{qubo} Hamiltonian as,
\[H=\sum_{i < j}^{N} J_{i,j}T_iT_j + \sum_{i=1}^{N}h_iT_i, \]
where in the case of triplets, $T_i$ represents $i^{th}$ triplet and $T$ can be mapped to the Pauli-Z operator on the $i^{th}$ qubit. $J_{ij}$ is the coupling coefficient between $i^{th}$ and $j^{th}$ triplets and $h_i$ determines the strength of the field on the $i^{th}$ triplet. Then, it is possible to use algorithms such as \gls{qaoa}, \gls{vqe} or \gls{hhl} to find the ground state of the Hamiltonian, which corresponds to the desired solution. Although such a \gls{qubo} Hamiltonian is sparse in general, it consists of at least $\mathcal{O}(10k)$ sites for a real world problem, or $\mathcal{O}(500)$ in more favourable scenarios with smaller occupancies such as the LHCb Vertex Locator. The limited number of qubits available currently restricts the Hamiltonian to $\mathcal{O}(1)$ sites and therefore strategies to partition the Hamiltonian to many smaller pieces are needed.

Recently, Ducket et al. proposed a method to solve the triplet classification using a \gls{qsvm} based approach~\cite{duckett_reconstructing_2022}. In this method, spatial coordinates of each hit from the triplet are encoded to quantum states that result in a 9-qubit circuit. Quantum kernel methods promise an advantage for datasets with many features, therefore a triplet based approach might not provide an advantage. However, this method may outperform a classical kernel method in cases where considering higher number of hits are useful.

Classical \gls{gnn} methods were shown to have linear scaling with respect to the number of input space-points, which makes them a strong candidate for future implementations of particle track reconstruction algorithms~\cite{ju_performance_2021}. Although there is no formal proof that this scaling is linear, the empirical evidence suggests so. It is likely that this improvement comes from the parallelisation capacity of \gls{gpu}s. This means that there is still a need for large \gls{gpu} clusters. A quantum advantage could be achieved if \gls{gnn}s with similar characteristics can be implemented on Quantum Computers. Recently, it was shown that a \gls{qgnn} approach is possible and it can perform similar to the classical equivalent for up-to 16 qubits~\cite{tuysuz21, QGNN-embedding}. However, understanding if this can be realized at large scale requires a larger number of qubits. 

The availability of a $100 \otimes 100$ device would enable the study of larger local Hamiltonians with 100 sites and give researchers a tool to investigate if \gls{qubo} based approaches are viable. Similarly, such a device would allow us to implement \gls{qgnn}s of sizes comparable to the classical state-of-the-art models. 

Regarding local methods, although a full analysis chain is presently unreachable due to hardware limitations, we can nevertheless consider a complexity analysis to illuminate the general evolution of the classical and quantum approaches to the problem. It is not clear, in particular, whether all steps in the track or, in general, object reconstruction may benefit from a quantum algorithmic approach.
This is the procedure originally followed, for instance, by Wei et al.~\cite{Wei2020}, which have estimated the classical and quantum computational scaling of a well-known (albeit unused) jet clustering algorithm. However, since this algorithm is not the current standard used at the LHC it is much more informative to estimate the complexity of a current choice, namely the \gls{ctf} algorithm~\cite{JINST}, which is the tracking algorithm used by the CMS collaboration~\footnote{New versions have been published~\cite{Bocci2020}, but the general analysis should hold also in this case.}. The underlying structure of the \gls{ctf}, the combinatorial Kalman filter~\cite{KalmanFilterFruhwirth}, is used by several current track reconstruction algorithms~\cite{Sguazzoni2016,ATLAS,Braun2018} and the analysis can easily generalized to most presently available algorithms. This program has been followed by in~\cite{Magano:2021jzd} using the algorithm as it is described in~\cite{JINST}. The conclusion is that it is possible to reconstruct the same tracks (up to bounded-error probability) with lower quantum complexity by an adequate use of quantum search routines.

A $100 \otimes 100$ machine may allow for some progress along the lines defined in~\cite{Magano:2021jzd}, although it is necessary to investigate the number of qubits which can effectively be used for the implementation of the program. Moreover, since~\cite{Magano:2021jzd} is applicable to a hybrid classical/quantum approach it is possible to implement the program according to the available resources. In any case, it is important to bear in mind that in the short-term track reconstruction algorithms will be quite limited by the input size, and investment in \gls{qaoa} or in jet clustering may be more rewarding. 


\paragraph{Jet Reconstruction and Identification}
\label{subsubsection:Jet reconstruction and identification}


Jet clustering algorithms aim at estimating the kinematics of the particle that initiated the jet. Usually, these algorithms are based on clustering schemes, which combines the observed particles into a jet for further study.

Clustering algorithms have different properties and characteristics that can make them more appropriate for a particular task, such as the extraction of observables or as a tool to extract specific properties of the final state. An essential property of an optimal jet clustering algorithm is \gls{irc} safety. An observable is \gls{irc} if it remains unchanged in the limit of a collinear splitting or the emission of an infinitely soft (low momentum) particle.

Two main approaches have been pursued in clustering particles into a jet: cone and sequential recombination schemes. The first approach aims to find regions with a high-energy flow and thus define rigid conic boundaries. In sequential recombination algorithms, particles are clustered locally using a distance metric.

Jet clustering algorithms can be computationally expensive, as the execution time scales polynomially with the number of particles to cluster. Speedups can be achieved by considering the clustering problem from a geometrical point of view instead of combinatorially. In this way, sequential recombination algorithms can be executed in $\mathcal{O}(N^2$) or even $\mathcal{O}(N \ln N$) complexity rather than $\mathcal{O}(N^3$). Cone algorithms could be implemented exactly (and therefore made \gls{irc} safe) with $\mathcal{O}(N^{2}\ln N$) rather than the expected $\mathcal{O}(N2^{N}$) complexity. 

Quantum-assisted algorithms have been explored to reduce the computational overhead of these clustering routines. The first application of quantum-assisted algorithms to the task of clustering particles into a jet was introduced in~\cite{Wei2020}. Two clustering techniques were put in place for the particular case of electron-positron collisions and inspired by the calculation of \textit{thrust}~\cite{BRANDT196457,PhysRevLett.39.1587}, an event shape quantity that allows for the partition of event particles into two hemisphere jets. The first approach targeted the universal quantum computing setting based on Grover's algorithm. In addition, a \gls{qubo} formulation for thrust was developed, suitable for quantum annealing. Classically, the calculation of thrust can be costly, scaling as $\mathcal{O}(N^3)$~\cite{YAMAMOTO1983597} for an event with $N$ particles, or using the improved method introduced in~\cite{Salam_2007}, as $\mathcal{O}(N^2 \log N)$. The thrust-based \gls{qubo} formulation was benchmarked in~\cite{DelgadoThaler2022}, using the D-Wave Advantage 1.1 QPU, and compared to classical \gls{qubo}-solving techniques such as simulated annealing and annealing optimization subroutines like reverse annealing. Results from these studies revealed the limitations of current quantum annealing devices in terms of connectivity. \gls{qubo} formulations involving many spin variables and all-to-all connectivity, like the thrust problem, perform poorly on currently available quantum annealers. An extension to the \gls{qubo} formulation for thrust calculation was presented in~\cite{pires2020adiabatic}, based on the angular distance between two particles in a given event and penalizing the assignment of two particles located on the same hemisphere of the partition. Results from the hardware deployment of these studies were limited to a low number of annealing runs due to limited access to the QPU.

Algorithms based on digital quantum computing have also been proposed; however, the algorithms in~\cite{pires2021digital, Wei2020} are not suitable for implementation on noisy devices due to the need for a QRAM-like architecture to access particle information in parallel. Another promising study~\cite{deLejarza:2022bwc} deals with the quantum version of three clustering algorithms found in the classical literature: \textit{k-means}~\cite{MacQueen1967} (a quantum version of this algorithm is used in Ref.~\cite{pires2021digital}), \textit{affinity propagation}~\cite{Brendan2007}, and the \textit{$k_T$} jet clustering algorithm~\cite{PhysRevD.48.3160}. Two quantum subroutines are introduced: the first computes the Minkowski distance between particles, and the second tracks the maximum in a long-tailed distribution. For both these subroutines, the authors prove polynomial speedups as compared to well-known classical algorithms.  The quantum algorithms were applied to simulated data for a typical LHC collision setting and obtained efficiencies comparable to their classical counterparts. In particular, the quantum-\textit{$k_T$} version is a conceptually more straightforward algorithm with a similar execution time compared to subroutines in the FastJet library~\cite{Fastjet}. 

Jet tagging, the identification of the flavor of the quark that originated the jet, is another aspect of jets physics that experimental physicists are continuously improving. For example, in the determination of the Higgs boson couplings to $b$ and $c$ quarks, the jet tagging efficiency and purity determine the actual size of the dataset useful for the measurements and, therefore, their accuracy. Jet tagging is based on global jet characteristics and on each jet's particle properties. In principle, it therefore requires a large number of features which means a high dimension dataset. 
The study reported in~\cite{btag} limited the data representation to a few properties to cope with the low number of qubits available and short circuit depth. As already mentioned in the introduction of this section, this is the approach often used in experimental \gls{hep}, and therefore the performance of the QC algorithms is by definition limited and the comparison with classical methods is performed adopting the same data-set dimension. 
Two different feature encodings have been tested: the angle encoding is used when a two feature data-set is used while for the 16 features the amplitude encoding is exploited.
Even though the exercise is quite simple, it showed that in the training phase, the \gls{qml} method reaches optimal performance with a lower number of events with respect to the classical ones. The limited access to the hardware resources did not allow an extensive study of the noise impact which was evaluated only for the 2 qubits case.
This study could largely benefit from much more powerful hardware, in particular the $100 \otimes 100$ IBM hardware.
Instead of re-proposing the same exercise, it would be possible to design a new circuit where the entanglement entropy can play an important role. In fact, jet tagging features correlation is considered as classical correlation while in \gls{qml} these can be understood and included in the circuit optimization improving the classification performance. 
A further step forward could be to perform the jet classification study on data obtained in proton-proton collisions and in Monte Carlo simulated events. Collider data may exhibit quantum characteristics,   not visible in simulation. That could happen due to the limited knowledge of jets formation and evolution which is regulated by the non-perturbative \gls{qcd} and described only by models in the simulation. Such effects, if there, are currently  absorbed by the systematic errors in the jet reconstruction quantities.


\subsubsection{Interpretable Models and Inference}
 
 In this section we review the use of quantum models as inference tools to extract the characteristic properties of a dataset in \gls{hep}. We give two examples of such models: characterising the non-perturbative structure of hadrons through \gls{pdfs}, and estimating the Wilson coefficients of \gls{efts} and their correlations. We emphasise the potential of these tools to enable precision modelling of physical phenomena and provide a first step towards being able to bridge the fields of quantum computing and quantum information in extracting quantum descriptors of \gls{hep} processes from models learned from data. 

In high-energy physics, perturbation theory is used to calculate particle interactions at high energies~\cite{Gross:1973id}. These perturbative methods allow for the calculation of scattering amplitudes as a series expansion in powers of the coupling constant. However, as the energy of the interaction decreases, the coupling constant becomes large and the perturbative expansion breaks down~\cite{Gross:1973ju}. This results in a non-perturbative regime where the underlying physical processes are not easily calculable, and must instead be obtained through experimental measurements or numerical simulations~\cite{Shifman:1978bx}. Characteristic functions that capture the essential features of the underlying physical process can be employed to represent the relative probability of a particular physical process as a sum of simpler, more tractable functions. The choice of basis functions and coefficients is critical in constructing an accurate representation of the underlying physical process as they must be able to accurately characterize the process in both the perturbative and non-perturbative regimes, as well as any additional physical constraints that may be present. 

\gls{pdfs} are an example of non-perturbative effects that are necessarily characterised by such approximations from data~\cite{collins_2011}. \gls{pdfs} describe the probability distribution of the momentum fraction carried by the quarks and gluons inside a proton. The need for \gls{pdfs} arises from the fact that the proton is a composite particle made up of quarks and gluons that are constantly interacting, which makes it impossible to calculate the momentum distribution of these partons using perturbative methods alone. Nonetheless, there are known constraints on the form of \gls{pdfs} that can be derived from the fundamental principles of quantum chromodynamics (\gls{qcd}), and their predictions are highly constrained by experiment.

The accurate estimate of \gls{pdfs} is vital to all measurements in experimental collider physics, as they are used to predict the rates and distributions of processes~\cite{practical_collider_physics}. 
Uncertainties arise from the limited precision of how experimental data is used to constrain the \gls{pdfs}, as well as from theoretical uncertainties in how to extract perturbative estimates from the fitted \gls{pdfs}.
Quantum computing affords us new avenues to address both of these shortcomings by providing characteristic functions that may better represent the nature of the process they are used to represent.

A recent study investigating an approach based on the use of a \gls{pqc} was explored for estimating the functional form of \gls{pdfs}~\cite{Perez-Salinas:2020nem} from data. The \gls{pqc} approach aims to find an ansatz for representing the \gls{pdfs} as a \gls{pqc}, the parameters of which are estimated using a classical optimization algorithm to minimise the difference between the predicted and experimental data. This is a promising avenue for leveraging the expressive power of \gls{pqc}s to efficiently learn solutions to classically intractable problems. Preliminary results using the \gls{pqc} approach are encouraging, showing good agreement with existing \gls{pdf} fits obtained through classical optimization techniques. This represents an exciting first step towards using quantum algorithms for \gls{pdf} estimation and highlights the potential of quantum computing for solving problems in high-energy physics. However significant work is still needed to leverage the quantum nature of the problem. In this construction, the \gls{pdfs} being estimated are still classical approximations to an inherently quantum system and as such the possible advantages of such a methodology are purely computational. It is foreseeable that quantum functions that characterise classically intractable processes such as these are possible in a way that, although simplified with respect to a fully numerical model (e.g. from lattice \gls{qcd}), would give a notable improvement over classical models when compared with data.

In contrast to experimental measurements in which the exact prediction of a given standard model process is computed, \gls{efts} provide a framework for modelling complex physical processes in terms of a hierarchy of simplified interactions, characterised by a set of Wilson coefficients~\cite{Lang:2021hnd}.
These coefficients represent the coupling strengths of various operators that encode the effects of high-energy physics and can be determined through a process of matching with experimentally measured observables.
While the precise values of the Wilson coefficients cannot be computed exactly, they can be approximated through a process of functional approximation, in which an ansatz is made for the form of the \gls{eft} such that the coefficients can be estimated from experimental data.
This \gls{eft} approach is similar in nature to the fitting of \gls{pdfs}, as both involve characterising complex physical phenomena in terms of a simplified set of paremetrised functions. 
In a recent study~\cite{Criado:2022aoo}, researchers proposed a new method for estimating Wilson coefficients using a quantum computer. The method involves using a quantum computer to encode the \gls{eft} predictions and experimental data into a \gls{qubo}~\cite{QUBO} optimization problem, with coefficients of the cost function determined by a Hamiltonian representation of a set of given coefficients. The \gls{qubo} problem is then solved on a quantum annealer to obtain the best-fit values of the Wilson coefficients. A primary goal of this method is that the optimization of the problem on a quantum computer can provide a more efficient and accurate way of estimating the Wilson coefficients than is possible with classical methods. 
The Hamiltonian is constructed using the parametrisation of an effective field theory approach, which allows for a systematic expansion in powers of the inverse of the mass scale of the new physics being probed.
The Standard Model Effective Field Theory (SMEFT) framework~\cite{SMEFT} used in this example contains a large number of parameters, making it challenging to extract information about the underlying physics. 
In this case the predictions for a reduced set of parameters are computed classically and only the relationship between the coefficients and the measurements modelled in the hamiltonian, not the dynamics of the \gls{eft} operators themselves.
Through a careful understanding of the Hamiltonian representation of this process and its solution using \gls{qml}, it might be possible to reduce the number of parameters needed to describe the system by identifying a smaller set of effective parameters that capture the essential physics and to identify correlations between different observables and effective couplings.

Whilst much of the development of quantum algorithms and in particular \gls{qml} is focused on identifying and solving computational bottlenecks present in traditional methodologies, the goals identified in these selected benchmark applications are those that leverage properties of \gls{qml} models to better interpret data from experimental high-energy physics in new ways. 
Several studies have begun towards this goal, however these initial steps provide only hints to which a complete understanding of how \gls{qml} can be used to leverage a quantum interpretation of the information contained in data from experimental particle physics.
 
 \subsubsection{Generative Models for Simulation}

 Other natural applications of generative modelling, are detector simulation and event generation.
Monte Carlo simulation of collider detector events is one of the most computing expensive tasks within the experiments data processing chain. Recent estimates suggest more than 50\% of the LHC  computing grid (WLCG) is spent on simulation tasks directly or on the simulated data reconstruction, i.e. the extraction of high-level features from simulated data~\cite{HSF}. The next generation detectors for the  High Luminosity LHC and future colliders, with their larger sizes, higher sensor granularity and increased complexity, will be even more demanding in terms of computing resources for data simulation and reconstruction~\cite{schmidt2016high}. 
This fact has sparked, over the years, intense research on alternative approaches, generally designated as fast simulation strategies in contrast to highly accurate Monte Carlo-based simulation. 

Fast simulation, typically trading some level of accuracy for speed, relies on parametric modelling of detectors response~\cite{ovyn2009delphes} or, more recently, on deep generative models~\cite{khattak2022fast,krause2021caloflow,buhmann2021getting} that learn multi-dimensional, conditional probability distributions.
In most cases, the focus is on the detector response itself: the deep generative models are trained to reproduce the detector output which is then processed in the same fashion as Monte Carlo simulated data. This approach can produce very realistic output, both in terms of quality of the individual events and in terms of sample diversity. In other cases direct generation of high level features, typically used at the analysis level, is preferred, thus skipping the entire reconstruction process~\cite{hashemi2019lhc}. This approach has the advantage of being computationally lightweight and flexible since the deep learning models learn directly the particles features and correlations in the final state of interest, taking into account all experimental effects. Its main limitation is the fact the output is inherently analysis specific, and cannot be used outside the scope it was initially designed for.

At the same time, several studies have started investigating quantum (or hybrid quantum-classical) implementations of generative models. A few examples are described in~\cite{arxiv.2203.08805} and references therein. In most cases quantum architectures inspired by classical models have been studied: for example, implementations of \gls{qgan} or \gls{qae}.  Particularly interesting is the case of \gls{qcbm}~\cite{qcbm} which instead are quantum generative models that do not have a classical counterpart and leverage the Born measurement rule during the sampling process. 
As in the classical domain, quantum architectures have been used to address two main types of applications: detector output simulation and final state generation. In both cases, but in particular for detector output simulation, the main limitation of the current models lies in the dimensionality of the simulated output. Realistic applications to particle physics detectors require generative models to learn distributions whose size scales with the number of detector sensors. 

 Current models can generate accurate simulations for very small (10-sized) setups, using one qubit to represent a detector sensor. Typically, reversible data compression techniques, such as auto-encoders,  can be used to bring down the original simulation to a size that is manageable by the quantum system: the classical encoder network produced a reduced latent representation which is then learned by the quantum generative model. A classical decoder network is then used to transform the synthetic output from the latent dimension to the original one. The expected advantage of using a quantum algorithm in this task would come from a more accurate and generalisable learning of the latent representation. It is clear, however that a most interesting development in this direction would require reducing the weight of the classical data dimensionality reduction step with respect to the quantum algorithm. In this context, a $100 \otimes 100$ machine would enable the simulation of far more realistic use cases.
 
Aside from the sheer detector size, the need for discretization also affects how realistic quantum generative models-based simulations can be. In most cases, our detectors produce continuous features, while qubits  naturally map to discreet quantities so the size of the qubit system can have an impact on the detector simulation resolution. 
These same problems can affect the direct generation of high-level features, albeit at a different scale: in this case quadri-momenta (and possibly angular correlations) of particles produced in scattering or decay processes are in the range of a few tens, instead of a few thousands, making the problem much more manageable on near term quantum systems. In this case, extreme care should be put into correctly describing cross-correlations among particles, thus good connectivity and the possibility to reproduce complex entanglement patterns over multiple qubits become essential.

