\section{Conclusions and Outlook\label{sec:conclusion_outlook}}

In this paper we have described applications from experimental and theoretical high-energy physics where quantum computing has the potential to show a better performance than their classical counterparts. The selected applications were chosen also with respect to IBM's 100$\otimes$100 challenge and, where possible, a resource estimate was made. We note that the given applications are by no means complete and should serve as examples which are of very high interest for the high-energy physics community. We emphasize that this work should serve as an initial step by the present authors for exploring the potential of quantum computing for high-energy physics and we expect that the community of high-energy physicists working on this will substantially grow in the future. 

Concerning the quantum algorithms proposed for the applications outlined in the theory section (\ref{subsect_Theory}), we have identified quantum dynamics as one of the main targets because of its relevance in the field of \gls{hep}, e.g., in scattering phenomena, string breaking, quenching or dynamical properties of phase transitions. 
In fact, the exponentially growing costs of the corresponding classical approaches combined with  
the availability of well-tuned quantum algorithms make quantum computing a very promising tool for tackling problems in quantum dynamics. 
As already outlined in the theory section in table~\ref{table:resources}, such quantum dynamics applications are indeed compatible 
with the $100 \otimes 100$ challenge. 
Besides the dynamical aspects of theoretical \gls{hep} models applications we have also described static situations where quantum computing could lead to a better performance. These include abelian and non-abelian lattice gauge theories supplied with topological terms or non-zero fermion density or investigations of neutrino oscillations. While for these cases quantum computing has clearly an advantage over classical Markov Chain Monte Carlo methods it remains to be seen, whether it will have advantages over tensor network approaches, e.g. when taking the continuum limit or close to a phase transition. 

In this paper, we have identified and proposed concrete examples of low (1+1)D and (2+1)D theoretical models of \gls{hep} (and in particular lattice gauge theory) which are particularly hard classically due to the level of the entanglement produced, but still preserve a great physical relevance as prototypes for understanding fundamental dynamic but also static aspects of the laws of Nature.  
In the path towards large-scale simulations, we propose the development of hybrid quantum-classical algorithms, which can optimally leverage the advantages offered by the two complementary computational paradigms; for instance, the combination of \gls{tn} with quantum circuit representation of the system wave function can offer a unique opportunity for enabling the simulations of strongly entangled systems for longer time scales or close to phase transitions.

We consider the here proposed models as an intermediate step towards eventually reaching (3+1)D theories as actually needed  for studying the standard model of \gls{hep}. Besides the fact that the here considered lower dimensional models are of a high interest by themselves, we are convinced that investigating them with quantum computing can significantly help to develop algorithms and methods for studying their (3+1)D counterparts. And, it is a really fascinating outlook to explore phases of \gls{qcd} where no one has looked before such as the very eaerly universe or when the strenghts of a topological term becomes large. In addition, it would allow to study scattering phenomena in a fully non-perturbative fashion opening completely new insight to the physics of article collisons and shed light on the transition of confined phase of \gls{qcd} to the quark gluon plasma. 

A wide variety of \gls{qc} applications are anticipated in quantum simulations and \gls{hep} experimental workflow, as described in the earlier sections. Quantum simulations of simplified \gls{lgt}s in the Standard Model, such as \gls{2p1D} \gls{qed} or \gls{2p1D} SU(2) theory, are potential, well-motivated applications for near-term quantum computers. For the experimental side, \gls{qml} is a major technique to exploit quantum computing in the applications such as signal processing and detector reconstruction. 
However, as mentioned in Section~\ref{subsect_Experiments}, when considering \gls{qml} for processing classical experimental data, the data encoding into a quantum circuit is a big challenge, in particular for future colliders where an enormous amount of data will be produced. Moreover, the data encoding is also known to be one of the critical processes that cause barren plateau. 

This motivates us to explore the possibility of utilizing {\it quantum data} in the future as a promising route to directly exploit quantum properties encoded in quantum simulation and \gls{hep} experimental data.
From a theoretical perspective, understanding the power of quantum data for learning quantum states has received a lot of attention. There has been a sequence of works in tomography, wherein a learner is given copies to an unknown $n$-qubit quantum state $\rho$ and needs to learn $\rho$ well-enough (up to $\varepsilon$-trace distance); here the sample complexity was pinned down~\cite{haah2017sample,o2016efficient} to $\Theta(2^{2n}/\varepsilon^2)$. 
However, the exponential nature of learning an unknown quantum state is undesirable; there have been works that have looked at restricted classes of states and shown that they are learnable using polynomially many copies of the states, such as stabilizer states~\cite{montanaro2017learning}, Gibbs states of local Hamiltonians~\cite{anshu2021sample}, matrix product states~\cite{CPFSGBLPL10}. Another body of work has considered the setting in which the goal is not to learn the entire unknown quantum state $\rho$ but to learn only certain properties of $\rho$. In this context, people have considered tasks such as $(i)$ PAC learning: the learning algorithm here is given access to $(E_i,\Tr(\rho E_i))$ where  $\{E_i,\mathbb{I}-E_i\}$ is a uniformly random \gls{povm} element, $(ii)$ Shadow tomography, where the goal is, given copies of an unknown quantum state $\rho$, can we learn the expectation values of $\rho$ with respect to a certain set of \emph{fixed, a priori known} observables $\{E_1,\ldots,E_m\}$, $(iii)$ Other models such as classical shadows, online learning, learning with differential privacy that have modified the models $(i,ii)$. In all these models of learning, it is well-established  that~\cite{aaronson:shadow,aaronson2018online,aaronson2007learnability} the complexity of learning is $\mathcal{O}(n)$, which is \emph{exponentially} better than tomography.  For a detailed survey on the complexity of learning quantum data, we refer the interested reader to~\cite{anshusurvey}.

More practical applications of quantum data learning to \gls{hep} is to use it for extracting physical information from quantum states in quantum simulation.
This was first proposed in the context of condensed-matter physics~\cite{2019NatPh..15.1273C}
and further explored in~\cite{2021arXiv210612627H,2021PRXQ....2d0321B,PhysRevA.102.012415,2021arXiv210903400S,bernien2017probing, 2021arXiv210306712B,monaco2023quantum}.
The typical example is a recognition of quantum phases, where the \gls{qml} model learns the pair of quantum states and their phases to predict phase of unknown states.
In the context of high-energy physics, we often encounter phase transitions that cannot be investigated by local order parameters, such as confinement/deconfinement transition in \gls{qcd}.
It would be interesting to apply quantum data learning method to extract physical information in such situations.


In the longer-term, one may perform quantum experiment, not only digital quantum simulation but also analogue quantum simulation or others, then measure the final states via quantum sensor, and transduce the states coherently to a quantum computer which performs \gls{qml} to extract physical information~(see e.g., \cite{Huang_2022}).
This hybrid system could be extended to the concept of quantum-enhanced \gls{hep} experiment. A fascinating direction to exploit quantum data is to physically place quantum sensing devices in experiments and directly feed quantum states registered on the sensors into quantum computers. This certainly involves many challenges, e.g., detect particles or wave-like matters in quantum sensor, coherently transfer the generated state to other quantum systems, perform quantum operations to measure physical properties within coherence time. Such experiments will, however, provide an exciting opportunity to directly explore quantum phenomena observed in \gls{hep} experiments and extract dynamical properties of entangled quantum states.

A `\textit{conditio sine qua non}' for the success of this program in the era of noisy, near-term quantum devices is the co-design of error mitigation schemes that can efficiently compensate for  the different noise sources (e.g., gate errors, qubit decoherence and cross-talk) and guarantee results of sufficient quality to extract the physics of interest. To this end, several error mitigation schemes have been proposed in the past few years (see Sec.~\ref{sec:ibm_roadmap}) including zero-noise extrapolation~\cite{Temme2017Error}, probabilistic error cancellation~\cite{Berg2022Probabilistic}, and the probabilistic error amplification approach recently applied to the dynamics of the transverse field Ising model with more than 100 sites~\cite{Eddins_2023}.
All these methods will require an accurate description of all noise sources of current devices, which in the mean-time became a very active and successful area of research~\cite{
Bennett1996Purification, 
Kern2005Quantum, 
geller2013efficient, 
Temme2017Error, 
Kandala2019Error, 
Berg2022Probabilistic}. 
Finally, the precision of most quantum algorithms will depend on the quality of the measurement process of the observables of interest. Accurate results can require a number of projective measurements that can easily exceed what is currently affordable with the present gate times (from about hundred $ns$ with superconducting qubits, up to a few hundred $ms$ with ion-based technologies), which determine the clock-speed of quantum computing hardware calculations. 
Also in this case, there is the urge to design novel approaches capable of reducing the measurement overhead. 
Informationally complete \gls{povm}~\cite{PRXQuantum.2.040342} as well as classical shadows~\cite{Huang2020} offer viable solutions to this problem, opening new avenues for the use of quantum computing in large scale simulations. 


