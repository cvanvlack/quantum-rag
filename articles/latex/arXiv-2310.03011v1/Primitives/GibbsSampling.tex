%!TEX root = ../main.tex

\begin{refsection}

\section{Gibbs sampling}\label{prim:GibbsSampling}

\subsubsection*{Rough overview (in words)}

Gibbs sampling is the task of preparing a quantum state in thermal equilibrium. This task is interesting in its own right as a means of testing the thermodynamic properties of quantum systems in a controlled way, but it is also a subroutine that is surprisingly useful within other quantum algorithms. Formally, given a Hamiltonian and a temperature, the task is to prepare the \textit{Gibbs state} (also known as the \textit{thermal state}) of that Hamiltonian at the associated temperature, or equivalently, to sample eigenstates of the Hamiltonian with probability proportional to their Boltzmann weights (motivating the name Gibbs \emph{sampling}).

Physically, Gibbs sampling is routinely achieved in experiments via cooling as a manifestation of open-system thermodynamics, although theoretical understanding of such processes has been largely heuristic. Computationally, quantum Gibbs sampling is the quantum analogue of the same classical task in the computational basis, often achieved by Markov chain Monte Carlo (MCMC) methods. As a representative example, the Metropolis--Hastings algorithm \cite{hastings1970monte} uses rejection sampling to construct a Markov chain whose stationary state is the classical Gibbs distribution; the Gibbs distribution can be efficiently sampled if the Markov chain mixes rapidly. Nowadays, Monte Carlo methods have already surpassed their original intent (Ising model simulation) and found ubiquitous applications in optimization and machine learning due to their simplicity and robustness. It is natural to wonder if the same features will be present for quantum Gibbs sampling.

Surprisingly, quantum algorithms and theoretical understanding of Gibbs sampling are severely underdeveloped. The most direct quantum algorithms for Gibbs sampling suffer from an explicit cost exponential in the size of the system. Another approach is to quantize classical Monte Carlo algorithms~\cite{temme2011quantumMetropolis}, but this approach has faced serious technical challenges rooted in quantum mechanics: the energy-time uncertainty principle (for imposing the Boltzmann weights) and the no-cloning theorem (for ``rejecting'' a quantum state). Recently, a new wave~\cite{chen2021fastThermalization, Shtanko2021AlgorithmsforGibbs, Rall_thermal_22,chen2023QThermalStatePrep} of proposals revisits the issue from the angle of open-system thermodynamics and gives nature-inspired algorithms for Gibbs sampling. These more directly emulate the dynamical process of thermalization and have the potential to achieve better runtimes for specific systems where thermalization is expected to be fast.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Rough overview (in math)}

Given a Hamiltonian $H = \sum_i E_i\ket{\psi_i}\bra{\psi_i}$ over $n$ qubits, a desired inverse temperature $\beta$, and an error parameter $\epsilon$, the Gibbs sampling task is to prepare an $n$-qubit quantum state $\rho$ such that
\begin{align}
    \nrm{ \rho - \sigma_{\beta} }_{\mathrm{tr}} \le \epsilon \quad \text{where}\quad \sigma_\beta
    :=\frac{ \mathrm{e}^{-\beta H }}{\mathcal{Z}}\propto \sum_i \mathrm{e}^{-\beta E_i} \ket{\psi_i}\bra{\psi_i} \quad \text{and} \quad \mathcal{Z} := \tr[\mathrm{e}^{-\beta H }].
\end{align}
The above uses the convenient error metric given by the trace norm $\lVert \cdot \rVert_{\mathrm{tr}}$, which controls the error for arbitrary bounded (possibly nonlocal) observables. In some applications, it could be sufficient to give a state $\rho$ that approximates all \textit{local} observables up to high precision, even if the global distance between $\rho$ and $\sigma_\beta$ is large. Note that $\sigma_\beta$ corresponds to an ensemble of eigenstates of $H$, where an eigenstate with energy $E_i$ occurs with probability proportional to the Boltzmann weight $e^{-\beta E_i}$.

To solve this problem, the quantum algorithm requires access to $H$, for example, through a \hyperref[prim:BlockEncodings]{block-encoding} of $H$. Block-encodings  can often be efficiently constructed, for instance, when $H$ is a sparse matrix or when $H$ is given as a sum of $\text{poly}(n)$ local interaction terms. Henceforth, assume that $H$ is offset such that it is guaranteed to be a nonnegative operator (no negative energies). 

An early approach \cite{poulin2009GibbsSamplingAndEval} for Gibbs sampling relied on \hyperref[prim:QPE]{quantum phase estimation} (QPE) and \hyperref[prim:AmpAmp]{amplitude amplification}. In particular, one starts with a $2n$-qubit maximally entangled state (for which the reduced density matrix on the first $n$ qubits is the maximally mixed state) and applies QPE to the first $n$ qubits, reading an estimate of the energy into an ancilla register. Under the simplification that QPE has perfect resolution, one now has the state
\begin{equation}
   \frac{1}{\sqrt{2^n}} \sum_{i} \ket{\psi_i} \ket{\phi_i} \ket{E_i}
\end{equation}
where $\ket{\psi_i}$ is the $i$th eigenstate of $H$, $E_i$ is the associated energy, and the states $\ket{\phi_i}$ form an arbitrary (unimportant) orthonormal basis. Next, one coherently rotates an ancilla qubit to put the correct Boltzmann weight into the amplitude:
\begin{equation}
    \frac{1}{\sqrt{2^{n}}}\sum_{i} \ket{\psi_i} \ket{\phi_i} \ket{E_i}\left(e^{-\beta E_i/2}\ket{0} + \sqrt{1-e^{-\beta E_i}}\ket{1}\right)\,.
\end{equation}
Note that the probability of measuring the final qubit in $\ket{0}$ is precisely $\mathcal{Z}/2^n$. Rather than measure and postselect, one now performs amplitude amplification on the ancilla being $\ket{0}$ to produce 
\begin{equation}
    \frac{1}{\sqrt{\mathcal{Z}}} \sum_i e^{-\beta E_i/2}\ket{\psi_i}\ket{\phi_i}\ket{E_i}
\end{equation}
up to small error, which is a purification of the Gibbs state $\sigma_\beta = \mathcal{Z}^{-1}\sum_i e^{-\beta E_i} \ket{\psi_i}\bra{\psi_i}$. While QPE does not exactly produce the operation described above, a more complete analysis in \cite{poulin2009GibbsSamplingAndEval,chiang2010quantum} shows the idea still works. This approach is akin to classical rejection sampling (see also \cite{ozols2013QuantumRejectionSampling}), where a state is chosen at random and accepted with probability $e^{-\beta E_i}$, such that repeating until acceptance yields a sample from the correct distribution. Due to \hyperref[prim:AmpAmp]{amplitude amplification}, the quantum algorithm enjoys a quadratic speedup. 

More advanced methods that have exponentially better $\epsilon$ dependence have since been developed. Reference~\cite{chowdhury2016QGibbsSampling} used a \hyperref[prim:TaylorDyson]{linear combination of unitaries} approach to perform the imaginary time evolution operator $e^{-\beta H}$, again followed by amplitude amplification. Technically, that work assumed access to an operator similar to $\smash{\sqrt{H}}$, but this requirement was removed in Gibbs samplers appearing in \cite{apeldoorn2017QSDPSolvers,apeldoorn2018ImprovedQSDPSolving}, which employ a method for implementing smooth Hamiltonian functions. Alternatively, one can use the \hyperref[prim:QSVT]{quantum singular value transformation} along with a polynomial approximation to the function $e^{-\beta(1-x)/2}$ on the interval $x \in [-1,1]$ \cite[Section 5.3]{gilyen2018QSingValTransf} and combine this with (fixed-point) \hyperref[prim:AmpAmp]{amplitude amplification} \cite{yoder2014FixedPointSearch}. 

Another family of quantum algorithms is closer in spirit to classical Monte Carlo methods. They quantize the Metropolis--Hastings algorithm (quantum Metropolis sampling~\cite{temme2011quantumMetropolis}) or simulate the dynamics arising from a system-bath interaction~\cite{chen2021fastThermalization, Shtanko2021AlgorithmsforGibbs, Rall_thermal_22,chen2023QThermalStatePrep}. These algorithms make fundamental usage of~\hyperref[prim:QPE]{quantum phase estimation} for probing the energy, but most importantly (and most nontrivially), they construct a detailed-balance ``quantum Markov chain'' via either discretely or continuously ``rejecting'' the quantum state. Care must be taken to perform the rejection step coherently and to handle the fact that the energies cannot be learned to infinite precision. 
Abstractly, Monte Carlo--style quantum algorithms emulate a discrete quantum channel (or a continuous Lindbladian) that converges to the Gibbs state after $\ell$ iterations
\begin{align}
    \mathcal{N}[\sigma_\beta] \approx \sigma_\beta\quad \text{and} \quad     \nrm{ \mathcal{N}^{\ell}[\rho_{0}] -  \sigma_{\beta}}_{\mathrm{tr}} \le \epsilon
\end{align}
for some initial state $\rho_0$. 
Like the classical Metropolis--Hastings algorithm, for some systems, the number of iterations $\ell$ for convergence can be exponentially large (or worse) in $n$, while for other systems, the number of iterations needed can be much smaller. It is a generally difficult problem to determine $\ell$, but it is expected that the size of $\ell$ will be related to the natural thermalization rate of the system. Note that such a process can be further quantized to gain quadratic speedup~\cite{wocjan2021Szegedy,chen2023QThermalStatePrep}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Dominant resource cost (gates/qubits)}

Assuming one has access to a \hyperref[prim:BlockEncodings]{block-encoding} of the Hamiltonian $H$, that is, a unitary whose upper left block is the operator $H/\alpha$, where $\alpha$ is a normalization constant at least as large as the spectral norm of $H$, one can accomplish the Gibbs sampling task using \cite[Lemma 44]{apeldoorn2017QSDPSolvers} (see also \cite[Corollary 16]{apeldoorn2018ImprovedQSDPSolving})
\begin{equation}\label{eq:Gibbs_complexity}
    \alpha\beta \sqrt{\frac{2^n}{\mathcal{Z}}} \cdot \text{poly}(\log(1/\epsilon), n)
\end{equation}
calls to the block-encoding and a similar number of other gates. 
Note that since we have assumed $H$ is non-negative, we have $\mathcal{Z}\leq 2^n$. In the case that $H$ is $d$-sparse, we can take $\alpha = d$. In the case one has access to $\sqrt{H}$, the $\beta$ dependence can be reduced from $\beta$ to $\sqrt{\beta}$ \cite{chowdhury2016QGibbsSampling}. This complexity statement might be regarded as a \emph{quadratic speedup} compared to the classical method of rejection sampling, which requires $2^n/\mathcal{Z}$ samples on average; however, note that this classical method only directly applies to diagonal (classical) Hamiltonians $H$. Otherwise, a classical approach may need to resort to exact diagonalization of $H$, which has $\bigO{2^n}$ space complexity and even worse time complexity. 

Monte Carlo--style quantum Gibbs sampling algorithms have complexity determined by 
\begin{equation}
\label{eq:cost_mixing_iteration}
    (\text{mixing time}) \cdot (\text{cost per iteration}). 
\end{equation}
The mixing time is expected to vary significantly for different systems of interest (based on classical Monte Carlo intuition), but for systems appearing in nature, one may be optimistic based on the observed fast thermalization of physical systems. The cost per iteration is dominated by the~\hyperref[prim:QPE]{quantum phase estimation} subroutine, which then scales with a certain energy resolution. An overall gate complexity can be roughly, e.g., $\text{poly}(n, \beta,1/\epsilon)$. However, as new algorithms are still being proposed, we do not give more concrete estimates of the complexity.  Indeed, to put together an end-to-end resource estimate, one needs to design better algorithms to reduce the cost per iteration as well as to estimate the mixing time (e.g., by exact diagonalization of the map for small system sizes). Of course, if Gibbs sampling is employed as a heuristic (as in many classical applications of Monte Carlo methods), the cost will be empirical.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection*{Caveats}

On the one hand, the superpolynomial $\mathcal{O}(\sqrt{2^n})$ complexity for Gibbs sampling that appears explicitly in Eq.~\eqref{eq:Gibbs_complexity} is necessary in general (for sufficiently large $\beta$ it allows one to solve NP-hard or even QMA-hard problems in the general case). On the other hand, most physical Hamiltonians (if they appear to thermalize in nature) should be simulable without exponential hidden prefactors.
The Monte Carlo--style approach to Gibbs sampling attempts to mimic nature more closely than the other algorithms with guaranteed complexities mentioned above; hence, it looks more promising for obtaining polynomial runtimes, but this must be verified through system-specific analysis or hardware demonstrations.


Finally, if the Hamiltonian comes from classical problems (such as \hyperref[appl:ConicProgramming]{solving semidefinite programs}), loading the instance may have exponential cost ($\mathrm{e}^{\Omega(n)}$), which in the above presentation is hidden in the assumption of a \hyperref[prim:BlockEncodingsClassical]{block-encoding of classical data}. Additionally, it is unclear whether systems arising from classical data, rather than underlying physical models, should be expected to ``thermalize'' quickly (i.e.~whether Monte Carlo--style algorithms converge in a small number of iterations).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Example use cases}

\begin{itemize}
    \item \hyperref[prim:MWU]{Multiplicative Weights Update} (MWU) method and \hyperref[appl:ConicProgramming]{conic programming}: Gibbs sampling is the main source of quantum speedup in the MWU method, which is used to solve semidefinite programs and other conic programs \cite{brandao2016QSDPSpeedup,brandao2017QSDPSpeedupsLearning,apeldoorn2017QSDPSolvers,apeldoorn2018ImprovedQSDPSolving,apeldoorn2019QAlgorithmsForZeroSumGames}. Existing analyses in this direction have employed Gibbs samplers with a guaranteed quadratic (but no larger) speedup, rather than the more heuristic and recent Monte Carlo--style algorithms.
    \item \hyperref[appl:QuantumChemistry]{Quantum chemistry}: An important step of estimating the ground state energy of electronic structure Hamiltonians is generating an ansatz state that has a large overlap with the ground state. This might be done via Gibbs sampling at sufficiently low temperatures; the overlap with the ground state is $e^{-\beta E_0}/\mathcal{Z}$. 
    \item \hyperref[appl:CondensedMatter]{Condensed matter physics}: Similar to quantum chemistry, Gibbs sampling provides a method for producing ansatz states for ground state energy calculation. Furthermore, condensed matter physicists are often interested in material properties at finite temperatures so that the Gibbs state can be equally interesting as the ground state itself. 
    \item Computing partition functions: One of the early references to develop quantum Gibbs samplers \cite{poulin2009GibbsSamplingAndEval} applied it to the problem of estimating the partition function $\mathcal{Z}$ up to small relative error. The partition function contains all the relevant thermodynamic information of the system.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \subsubsection*{Further reading}

Gibbs sampling has been studied in several specific cases. For example, ~\cite{bilgin2010PreparingThermalStates} studied Gibbs sampling of local Hamiltonians in 1D. Moreover, \cite{kastoryano2016GibbsSamplingCommutingCase} studied \textit{commuting} spatially-local Hamiltonians and showed conditions under which they thermalize in polynomial time, suggesting efficient Gibbs sampling via Monte Carlo--style methods. These conditions hold for any 1D system at any temperature, and in any higher spatial dimension above a certain threshold temperature.

%%
\printbibliography[heading=secbib,segment=\therefsegment]

\end{refsection}