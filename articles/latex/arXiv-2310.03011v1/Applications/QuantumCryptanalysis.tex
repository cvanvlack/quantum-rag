%!TEX root = ../main.tex

\begin{refsection}
    
\section{Cryptanalysis}\label{appl:cryptanalysis}

Computation and communication are secured by cryptography. For example, a user's data can be made private, along with messages that they send or receive, from malicious agents who interfere to try to learn the sensitive information. A set of algorithms collectively called a \emph{cryptosystem} endows the security. The attempt to break security is known as \emph{cryptanalysis}, which has its own set of algorithms. Historically, both cryptography and cryptanalysis considered classical, polynomial-time algorithms as the only realistic ones. The advent of quantum computation forces us to consider attacks via quantum algorithms. Generally, we want to know what is the best algorithm for cryptanalysis, in order to understand the effect on the cryptosystem in the worst case. The effect of quantum attacks can be to void the security of a set of widely used cryptosystems (section on \hyperref[appl:BreakingCrypto]{breaking cryptosystems}). More broadly, quantum cryptanalysis can reduce a cryptosystem's security (section on \hyperref[appl:WeakeningCrypto]{weakening cryptosystems}), such that it becomes more expensive to implement in a secure manner. While the properties of quantum mechanics can also be used to devise more secure cryptosystems (e.g.,~quantum key distribution)~\cite{bennett1984QKD,Pirandola2020QKD,xu2020QKD}, we consider this area of cryptography to be outside the scope of the present discussion on quantum algorithms. 

\localtableofcontents

\printbibliography[heading=secbib,segment=\therefsegment]
\end{refsection}
\newpage

\begin{refsection}

\subsection{Breaking cryptosystems} \label{appl:BreakingCrypto}

\subsubsection*{Overview}
Much of modern cryptography relies on computational assumptions.\footnote{An example of a cryptosystem not requiring computational assumptions is the one-time pad.} A cryptosystem is considered secure if, assuming that a particular mathematical problem is hard to solve, an adversary cannot learn more than a negligible amount of information about what is being encrypted. The earliest such cryptosystems used particular problems from number theory, and variants are widely deployed to this day~\cite{katz2021IntroCryptography}. These cryptosystems are in the class of public-key cryptography, which enables any user to perform tasks like encryption, in contrast to symmetric cryptography, in which users have to pre-share a secret key.

Quantum computers use quantum algorithms to solve computational problems, and in some cases they provide a speedup over the best known classical techniques. When they are applied to the underlying computational task in a cryptosystem, a large speedup over classical methods can break the cryptosystem, in that an adversary efficiently learns the encrypted information to a non-negligible degree. One of the first discovered and most famous applications of quantum computing is Shor's algorithm~\cite{shor1994Factoring}, which breaks common methods of public-key cryptography based on number theory, including factoring, discrete logarithm, and elliptic curves. The applications of these public-key cryptosystems include encryption to hide the contents of a message, signatures that prevent tampering and impersonation, and key exchange to generate a key for symmetric cryptography~\cite{bernstein2017PostQuantumCrypto}. In this section, we restrict our focus to two of the most widely used cryptosystems: Rivest--Shamir--Adleman (RSA) and elliptic curves.

\subsubsection*{Actual end-to-end problem(s) solved}

The RSA cryptosystem~\cite{rivest1978RSA} relies on a user choosing a large number $N$ that is the product of two prime numbers; arithmetic is done modulo $N$. Denote by $n=\lceil \log_2(N) \rceil$ the number of bits specifying $N$. The two prime numbers are kept private, but their product is publicly revealed, along with an exponent $e$. A message $m$ is encrypted as $m^e \bmod N$. By construction, using tricks from number theory, there exists $d$ such that $(m^e)^d \bmod N = m \bmod N$. That is, exponentiating with $d$ performs decryption. The user efficiently solves for the necessary value of $d$ using the Euclidean algorithm, by knowing the prime factors of $N$, along with $e$. However, if an adversary is able to find the factors of $N$ after the construction by the user, they can also solve for $d$ and thereby decrypt messages. The security of the cryptosystem comes from the difficulty of factoring large numbers, i.e., finding the two primes that multiply to $N$. 

A similar cryptosystem is based on elliptic curves, which has the advantage that classical algorithms attacking it are even less successful than for RSA, so the ratio of bits of security (quantifying the number of attacks needed to learn the encrypted information; see section on \hyperref[appl:WeakeningCrypto]{weakening cryptosystems} for details) relative to key size is larger. Consequently, fewer resources (e.g., communication, complexity of encryption and decryption) are required to implement elliptic curve cryptography. Instead of using the multiplicative group of a finite field, consider points on an elliptic curve~\cite{koblitz1987ECC,miller1986ECC}:
\begin{equation}
y^2=x^3+ax+b\,,\quad a,b\in K\,,
\end{equation}
where $K$ is a field. A special group operation can be defined over points $(x,y)$ lying on the elliptic curve. Then, given a secret number $c$ and a point $P = (x,y)$, the point $P$ can be added to itself under this operation $c$ times, yielding the point $P'=cP$, which can be efficiently computed from $c$ and $P$.
Multiplication by $c$ is analogous to the exponentiation in RSA, above. The assumption of hardness is in the following problem, known as the elliptic curve discrete logarithm problem (ECDLP): \emph{For two points $P,P'$ on an elliptic curve, find an integer $c$ such that $P'=cP$}. As an example of this cryptosystem, for a publicly known point $P$, a receiver chooses a secret $c$ and publishes $cP$. The sender chooses a random integer $d$ and encrypts the message $m$ as $m+ d(cP)$, also sending $dP$. Since group multiplication is commutative, to decrypt the message, the receiver multiplies $dP$ by $c$ and subtracts the product from the encrypted message.

\subsubsection*{Dominant resource cost/complexity}

Shor's algorithm~\cite{shor1994Factoring} solves the number-theoretic problem of order finding: given $n$-bit positive integer $N$ and $x$ coprime to $N$, find the smallest integer $r$ such that $x^r=1 \bmod N$. Factoring was shown to reduce to order finding. In particular, there is an efficient, otherwise classical algorithm, of classical complexity $\bigO{n^3}$~\cite{nielsen2002QCQI}, that uses order finding as a quantum subroutine. To describe the quantum algorithm for order finding, let the function $f$ denote modular exponentiation, i.e.,~$f(e) = x^e \bmod N$, and note that $f$ is periodic with (unknown) period $r$. Also, let $L$ be a large integer such that an interval of length $L$ contains many periods, i.e.,~$L \gg r$. It can be shown that $L \geq N^2$ is sufficient. There are three steps. First, an equal superposition over the numbers $\{0,\ldots,L-1\}$ is formed and the function $f$ is computed into an ancilla register yielding the state $L^{-1/2}\sum_{e=0}^{L-1} \ket{e}\ket{f(e)}$. Second, a measurement is performed on the ancilla register, which, due to the periodicity of the function $f$, yields a state $(\lceil L/r \rceil)^{-1/2} \sum_{j=0}^{\lfloor L/r \rfloor}\ket{rj + y}$ for $0\leq y<r$ a random and unknown integer.\footnote{If $r\lfloor L/r \rfloor+y \geq L$, then the $j = \lfloor L/r \rfloor$ term does not appear in the expression.} Third, a \hyperref[prim:QFT]{quantum Fourier transform} is performed. In the case that $L$ is a multiple of $r$, the result is 
\begin{equation}
  \frac{\sqrt{r}}{L}\sum_{j=0}^{L/r} \sum_{z=0}^{L-1}e^{2\pi i z(rj+y)/L} \ket{z}  = \frac{1}{\sqrt{r}}\sum_{\ell=0}^{r-1}e^{2\pi i \ell y/r} \ket{\ell L/r } \,,
\end{equation}
where the equality follows since coefficients of $\ket{z}$ for which $z$ is not equal to $\ell L/r$ for some integer $\ell$ vanish due to destructive interference. Measurement of this state then produces an outcome $\ell L/r$ for a random choice of $\ell$. The value of $r$ can be classically computed by dividing the measurement outcome by $L$ and determining the value of the denominator of the rational number that results; repetition may be required since $\ell $ and $r$ could have common divisors. If $L/r$ is not an integer, the measurement outcome is (with high probability) an integer close to $\ell L / r$ for some integer $\ell$. One can deduce the rational number $\ell/r$ (which allows for the determination of $r$) from the estimate of $\ell L /r$ by writing it as a continued fractions expansion, with classical complexity $\bigO{n^3}$~\cite{nielsen2002QCQI}.

This entire procedure can alternatively be viewed as \hyperref[prim:QPE]{quantum phase estimation} applied to the unitary $U$ that sends $\ket{y}\mapsto \ket{xy \bmod N}$ for all $y$ relatively prime to $N$, performed with at least $2n$ bits of precision. 

The number of qubits for order finding is $\bigO{n}$, which stems from the number of bits specifying the problem: the first register has size $2n$, and the ancilla register holding the result $f(e)$ has size $n$.  Naively, the number of operations is $\bigO{n^2}$ for the quantum Fourier transform and $\bigO{n^3}$ for implementing the coherent modular exponentiation $\ket{e}\ket{0} \mapsto \ket{e}\ket{x^e \bmod N}$. The bottleneck in the complexity is thus from reversible circuits for modular arithmetic. These circuits are closely related to those in classical computing that have been optimized. The best scaling in theory is achieved with algorithms that have large prefactors in their complexity, making them impractical to implement except for large numbers: $\bigO{n^2\log(n)}$ is possible asymptotically, using integer multiplication with $\bigO{n\log(n)}$ scaling~\cite{harvey2021IntegerMultiplication}. Alternatively, optimization may be performed to, e.g., increase qubit count and decrease gate count. For example, an approximate version of the quantum Fourier transform is implemented with $\bigO{n\log(n)}$ gates and allows factoring with $\bigO{\log (n)}$-depth quantum circuits~\cite{cleve2000FastQFT}, at the cost of extra overhead in number of qubits and gates; allowing for $\bigO{\log^2 (n)}$-depth preserves the circuit size $\bigO{n^3}$.

A related approach proposed by Regev~\cite{regev2023efficient} for quantum factoring has quantum circuit size of only $\bigOt{n^{3/2}}$ gates but the circuit has to be run $\bigO{n^{1/2}}$ times. Furthermore, the algorithm relies on a plausible number-theoretic assumption. The reduction in quantum circuit size may lead to more favorable resource counts in practice.

Essentially the same quantum algorithm of Shor is readily applied to elliptic curves, as well as the discrete logarithm problem (i.e., find $r$ such that $a^r=b$ for $a,b\in G$ where $G$ is a group) that also is used as a computationally hard problem for cryptography. 
These applications are all instances of the \emph{hidden subgroup problem}: Find the generators for subgroup $K$ of a finite group $G$, given a quantum oracle performing $U\ket{g}\ket{h}=\ket{g}\ket{h \oplus f(g)}$, where $f:G\to X$ ($X$ is a finite set) is a function that is promised to be constant on the cosets of $K$ and take unique values on each coset. In the case of period finding, $G$ is the group $\mathbb{Z}/L\mathbb{Z}$ under addition, and the hidden subgroup is $K = \{0,r,2r,\ldots,L-r\}$ (technically a subgroup only if $r$ divides $L$); one can verify that $f(g) = x^g \bmod N$ is constant on each coset of $K$. The procedure outlined above for period finding can be applied to other groups, where it is called ``the standard method'' \cite{childs2010QAlgosForAlgebraicProblems} (which requires generalizing the \hyperref[prim:QFT]{quantum Fourier transform} to arbitrary groups). For abelian groups, the hidden subgroup $K$ can be determined with $\mathrm{polylog}(|G|)$ queries to $f$, but the method does not work for nonabelian groups, such as the symmetric group and the dihedral group.


\subsubsection*{Existing error corrected resource estimates}

The minimum recommended key size for RSA is 2048 bits~\cite{barker2015KeyRecommendation}. Optimizations in the circuits~\cite{beauregard2003ShorCircuit,haner2017factoringToffolis} and incorporation of hardware constraints~\cite{fowler2012SurfaceCodes} have led to decreasing but also more realistic resource estimates. For key size 2048, assuming nearest-neighbor connectivity, about $14000$ logical qubits (which includes space for routing and distillation; see sections on \hyperref[prim:QEC]{quantum error correction} and \hyperref[prim:LatticeSurgery]{lattice surgery}) and $3\times 10^9$ Toffoli gates are necessary~\cite{gidney2021HowToFactor}. 

For elliptic curve cryptography, the minimum recommended key size to ensure 128-bit security, is 256 bits~\cite{barker2015KeyRecommendation} (achieving the same level of security with RSA requires a key size of 3072 bits~\cite{boudot2020Factorization240Digit,roetteler2017ResourceEstimatesEllipticCurve}). 
For breaking 256-bit elliptic curve cryptography, it is estimated that around three times fewer logical qubits, and 100 times fewer Toffoli gates are required (compared to 3072-bit RSA)~\cite{roetteler2017ResourceEstimatesEllipticCurve}. Similar to factoring, improvements have been made in circuit compilation~\cite{haner2020ImprovedEllipticCurve} and hardware considerations~\cite{webber2022HardwareSpecifications}, resulting in an estimate of 2871 logical qubits and $5.76\times 10^9$ $T$ gates (note that one Toffoli gate costs around 4 $T$ gates). As a conclusion, breaking elliptic curve cryptography is easier than factoring for quantum computers in practice~\cite{proos2003ShorEllipticCurves}, relative to their practical difficulty on classical computers. 

In both cases (2048-bit RSA~\cite{gidney2021HowToFactor,ha2022ShorResources} and 256-bit elliptic curves~\cite{webber2022HardwareSpecifications}), given current hardware schemes based on surface codes, the number of physical qubits is estimated to be on the order of $10$ million and the computation runs for around $10$ hours. For a discussion on how to convert between logical and physical resources, see the section on \hyperref[prim:FTQC]{fault-tolerant quantum computation}. Optimization based on the particular architecture can give improvements to these estimates. For example, assuming a logarithmic number of nonlocal links, as in photonic implementations, enables breaking elliptic curves around 200 times faster~\cite{litinski2023EllipticCurvesBaseline}. The algorithms considered in the resource estimates above do not achieve the best known asymptotic scaling, which comes at the cost of large constant prefactors.


\subsubsection*{Caveats}
While the popular cryptosystems based on number-theoretic problems are rendered insecure for public-key cryptography, there exist alternatives that are believed to be secure against quantum computers: e.g., based on error-correcting codes or lattices~\cite{bernstein2017PostQuantumCrypto}. These alternative computational problems are believed to be hard for both classical and quantum computers.
The National Institute of Standards and Technology (NIST) of the United States plans to provide standards by 2024 to prompt implementation~\cite{alagic2022PostQuantumCryptoStatus3}. The class of symmetric cryptography (see a standard text~\cite{katz2021IntroCryptography} for details) involves computations that do not have much structure, and also is not broken by quantum computers. Instead, \hyperref[appl:WeakeningCrypto]{the number of bits of security is reduced}.

Prior experimental demonstrations of Shor's algorithm have used knowledge of the answer in order to optimize the circuit and thus lead to sizes that are experimentally feasible on non-error-corrected devices. Meaningful demonstration should avoid such shortcuts~\cite{smolin2013OverQF}, which are not available in realistic cryptographic scenarios.


\subsubsection*{Comparable classical complexity and challenging instance sizes}
The best known classical algorithm for factoring is the number field sieve, which has time complexity super-polynomial in number of bits $n$: namely, it scales as $\bigO{\exp(p\cdot n^{1/3}\log^{2/3}(n))}$, where $p>1.9$. With a hybrid quantum-classical algorithm applying \hyperref[prim:AmpAmp]{amplitude amplification} on the number field sieve, $p= 1.387$ can be achieved using a number of qubits scaling only as $\bigO{n^{2/3}}$~\cite{bernstein2017LowResourceFactoring}. Classically, problems of size 795 bits have been factored, taking 76 computer core-years, which distributed in parallel over a cluster took 12 days; the same team then extended the record to 829 bits~\cite{boudot2020Factorization240Digit}.

Several algorithms attacking elliptic curve cryptography have complexity $\bigO{2^{n/2}}$~\cite{washington2003elliptic}, leading to the recommended doubling of key size compared to bits of security. In practice, a problem of size 117 bits was solved~\cite{bernstein2016ECCFPGA}.

\subsubsection*{Speedup}
The number of gates to implement Shor's algorithm is $\bigOt{n^2}$ asymptotically using fast multiplication on large numbers~\cite{beckman1996EfficientNetworksFactoring}. More practically, without incurring the time overhead and additional storage space of fast multiplication, the scaling is $\bigO{n^3}$. Assuming classical and quantum gates are polynomially related in time complexity, the speedup is super-polynomial. However, there are no tight lower bounds on the classical complexity of factoring or ECDLP; it remains possible that more efficient classical algorithms could be discovered.

\subsubsection*{NISQ implementations}

The large circuit depth, complicated operations, and high number of qubits needed to implement Shor's algorithm make faithful NISQ implementation challenging. However, there have been several attempts to ease implementation at the expense of losing the guarantees of Shor's algorithm, in the hope that the output is still correct with some nonzero probability, which could be vanishing.

One approach~\cite{rossi2022ReducedShor} is to simplify several operations and make them approximate. The outcome is that the circuit depth is $\bigO{n^2}$, saving a factor of $n$~\cite{haner2017factoringToffolis}. The depth is then about $10^8$ to factor a 1024-bit instance of RSA, so for relevant sizes, error correction is still required. Implementation of the approximate algorithm, including experimentally, allowed for the successful factorization of larger problem instances than had been possible before. This approximate version is not NISQ in the usual sense of involving noisy circuits, but rather introduces some uncontrolled approximation error in return for reducing the depth, for the possibility of a useful result. Another approach is to encode the factoring problem in a \hyperref[prim:VQA]{variational optimization circuit}. Again, performance is not guaranteed; moreover, variational optimization applied to generic problems is expected to have, at best, a quadratic improvement compared to classical methods, leaving no hope for breaking cryptography. Classical simulation on small problem sizes shows that the algorithm can succeed~\cite{anschuetz2019variationalfactoring}, as does experimental implementation on a superconducting quantum processor~\cite{karamlou2021VariationalFactoringSuperconducting}. We emphasize that, generally, these NISQ approaches have no evidence or arguments for scaling to cryptographically relevant system sizes.

\subsubsection*{Outlook}

The existence of Shor's algorithm implies common RSA and elliptic curve schemes are theoretically not secure, and resource estimates have made clear what scale of quantum hardware would break them. While such hardware does not exist currently, progress towards such a device can be used to inform the speed of transitioning to quantum-resistant encryption~\cite{chen2016PostQuantumCrypto}. Currently, from a hardware perspective, the field of quantum computing is far from implementing algorithms that would break encryption schemes used in practice. The estimates above suggest that the resources required would be millions of physical qubits performing billions of Toffoli gates running on the timescale of days. In contrast, current state-of-the-art is on the order of one hundred noisy physical qubits, with progress towards demonstration of a single logical qubit. Running fault-tolerant quantum computation requires extra overhead, such as magic state factories (see the sections on \hyperref[prim:QEC]{quantum error correction} and \hyperref[prim:LatticeSurgery]{lattice surgery}). Thus, the gap between state-of-the-art hardware and the requirements for breaking cryptosystems is formidable. Moreover, a linear increase in key size will increase, e.g., the number of Toffoli gates by a power of three, which can be substantial. Therefore, considering the experimental challenges, likely only the most sensitive data will be at risk first, rather than common transactions. Consequently, these highly confidential communications will likely adopt post-quantum cryptography first to avoid being broken. However, insecure protocols often linger in practice, so quantum computers can exploit any vulnerabilities in deployed systems that have not been addressed. For example, RSA keys of size 768 bits have been found in commercial devices (note that such key sizes can already be broken classically~\cite{boudot2020Factorization240Digit}). In addition, intercepted messages, encrypted with RSA or elliptic curves, can be stored now and decrypted later, once large-scale quantum computers become available.

The resilience of candidates for post-quantum cryptography is under active investigation. In particular, specialized quantum attacks~\cite{peikert2020CSievesCSIDH} can reduce the number of bits of security, \hyperref[appl:WeakeningCrypto]{weakening} the cryptosystem. Classical attacks have even broken certain cryptosystems~\cite{castryck2023EfficientSIDH}. Note that these attacks affect the feasibility of particular proposals, but there exist other post-quantum candidates that have no known weaknesses.

A sensitive area that warrants additional discussion is cryptocurrency, since much of the encryption relies on the compromised, number-theoretic, public-key cryptography. Moreover, changing the cryptographic protocol of the currency requires that most of the users reach a consensus to do so, which can be challenging to coordinate, even if the technical hurdles of adopting post-quantum encryption are overcome. Cryptocurrency wallets that have revealed their public key (for example, via a transaction reusing a public key assigned to that wallet previously) can be broken using Shor's algorithm. An attack is also possible during the short time-window in which the key is revealed during a single transaction~\cite{aggarwal2018QuantumAttacksBitcoin}. Different cryptocurrencies have different levels of susceptibility to these types of attacks~\cite{deloitte2019Bitcoin, deloitte2022ethereum}. Nevertheless, the mining of cryptocurrency is not broken, but only \hyperref[appl:WeakeningCrypto]{weakened by quantum computers}.

\printbibliography[heading=secbib,segment=\therefsegment]
\end{refsection}

\newpage



\begin{refsection}
\subsection{Weakening cryptosystems} \label{appl:WeakeningCrypto}

\subsubsection*{Overview}
The discovery of Shor's algorithm (see \hyperref[appl:BreakingCrypto]{Breaking cryptosystems}) prompted interest in post-quantum cryptography, the study of cryptosystems assuming the presence of large-scale, working quantum computers~\cite{bernstein2017PostQuantumCrypto}. While some existing systems retained confidence in their security, others that were broken by quantum algorithms were superseded by those that accomplish the same task, but are believed to maintain a high level of security against quantum attacks. 

Even if a cryptosystem is not broken altogether, its degree of security can be weakened by quantum algorithms. The strength of a cryptosystem is typically quantified by the number of bits of security, i.e., $n$ bits corresponds to guessing the desired information with probability $1/2^n$ and accessing what is being protected. \textit{Breaking} a cryptosystem means only an efficient number of attempts (i.e., $\mathrm{poly}(n)$) are needed, while an attack that \textit{weakens} a cryptosystem still takes $2^m > \mathrm{poly}(n)$ attempts, for some $m<n$. 

In contrast to public-key cryptosystems, symmetric-key cryptography was discovered earlier and has fewer capabilities. However, it relies less on the presumed hardness of underlying mathematical problems, and correspondingly has only been weakened by quantum cryptanalysis, as discussed in more detail below.

\subsubsection*{Actual end-to-end problem(s) solved}

In symmetric-key cryptography, two communicating parties share the same key $K$, which is used both in encryption $\mathit{Enc}_K$ and decryption $\mathit{Dec}_K$. As usual, the cryptographic algorithm $(\mathit{Enc}_K,\mathit{Dec}_K)$ is known to everyone, including adversaries. Then, the task of the adversary is to learn the key, given access to $r$ pairs of plaintext (the message $m$) and corresponding ciphertext $c$ (its encryption). Such a pair can be accessed by, e.g., forcing a certain test message to be transmitted. Precisely, an input $K$ is sought for which the following function outputs 1:
\begin{equation}
    f(K) = (\mathit{Enc}_K(m_1)=c_1 \land \ldots \land \mathit{Enc}_K(m_r)=c_r) \,, 
\end{equation}
i.e., find a key such that all the messages encrypt correctly. A straightforward attack is to use brute force and test every key; in practice, sophisticated classical attacks do not perform better than this approach in asymptotic scaling. 


\subsubsection*{Dominant resource cost/complexity}

The main, generic quantum attack is to use \hyperref[prim:AmpAmp]{amplitude amplification}: given a classical algorithm with success probability $\bigO{2^{-n}}$ of finding a solution, the probability is increased quadratically to $\bigO{2^{-n/2}}$. Thus, applying amplitude amplification to the task of solving for the key, the security of cryptosystems goes from $n$ bits to $n/2$.

The function queried in superposition must be efficient to evaluate with a quantum circuit, which is often the case in cryptography~\cite{bernstein2017PostQuantumCrypto}. However, the operations are typically long sequences of Boolean arithmetic. As such, a universal gate set and fault-tolerant computation are still required. To store the key, $\bigO{n}$ register qubits are needed, and many more ancilla qubits are used for the reversible arithmetic.

\subsubsection*{Existing error corrected resource estimates}
Consider the Advanced Encryption Standard (AES)~\cite{nist2001AES}, a symmetric encryption algorithm that is widely used in cryptosystems, e.g., for encrypting web traffic. At a high level, it mixes the plaintext and adds it to the key to obtain the ciphertext. An attack based on amplitude amplification needs around 3000--7000 logical qubits~\cite{grassl2016GroverAESResourceEstimates} for AES-$k$, where $k$ denotes key size in bits, and $k\in \{128,192,256\}$. For these sizes, the number of necessary problem instances $r$ is three to five. While the number of logical qubits roughly doubles going from AES-128 to AES-256, the number of $T$ gates goes from $2^{86} \approx 10^{25}$ to $2^{151} \approx 10^{45}$.

\subsubsection*{Caveats}
Since the quantum attack only halves the exponent in the complexity, a simple fix is to double the key length, e.g., adopting AES-256 instead of AES-128. This modification results in increased, but usually tolerable, cost in implementation (i.e., complexity of encryption and communication resources). In addition, there exist cryptosystems with an information-theoretic security guarantee, assuming adversaries with unlimited computational power, which covers against quantum attacks~\cite{bernstein2017PostQuantumCrypto}.

Furthermore, it is important to note that to realize the full quadratic benefit of \hyperref[prim:AmpAmp]{amplitude amplification}, the $2^{n/2}$ function queries must be performed in series. In contrast, classical brute-force attacks can exploit the parallelism available in high-performance classical computers, potentially increasing the value of $n$ for which a quantum approach would be advantageous over classical methods.

\subsubsection*{Comparable classical complexity and challenging instance sizes}
Classical algorithmic attacks on AES have reduced the security by only a few bits~\cite{bogdanov2011BicliqueAES}. More practical are side-channel attacks, which make use of physical byproducts, such as energy consumption. For example, when comparing bits between a key and another string, a flipped value can result in logic that increases energy consumption, compared to the same value where nothing happens. The two cases are distinguished and information about the key is learned. 128 bits of security is currently about the minimum recommended amount~\cite{barker2020KeyRecommendation}.

\subsubsection*{Speedup}
The basic speedup is quadratic: $\bigO{\sqrt{N}}$ function evaluations compared to $\bigO{N}$ classically, where $N$ denotes the number of possibilities for the key; i.e., $n=\lceil \log_2(N) \rceil$. However, the function queries in amplitude amplification cannot be parallelized. Then, the evaluation time of the function sets a bottleneck~\cite{bernstein2017PostQuantumCrypto}. That is, the problem size is limited by the number of function evaluations $T$ that can be run in an acceptable period of time. For $\sqrt{N}>T$, employing $p$ parallel quantum processors, each executes $T=\sqrt{N/p}$ evaluations. Then, $p=\bigO{N/T^2}$ and the total number of evaluations is $pT=\bigO{N/T}$, whereas classically, the number of processors is $\bigO{N/T}$ and total evaluations is $\bigO{N}$. The advantage is a factor of $T$, which is the bottleneck, rather than the larger $\sqrt{N}$. However, the advantage can be overshadowed by faster or cheaper classical processing. That is, if classical computers evaluate the function $T$ times faster than quantum processors, there is no time-advantage with using the quantum device. Furthermore, this argument assumes the same cost of parallelization for classical and quantum, which is optimistic for quantum devices. An example of this effect is in mining cryptocurrency~\cite{aggarwal2018QuantumAttacksBitcoin}: while a quantum computer needs quadratically fewer attempts to succeed, the development of fast, specialized, classical hardware negates the advantage.

\subsubsection*{NISQ implementations}

The key can be encoded as the ground state of a Hamiltonian, and then \hyperref[prim:VQA]{variational methods} are applied to solve for it. The scaling is expected to be the same as amplitude amplification. However, since the variational algorithm does not have a set time-complexity, the solution may be found much slower or faster~\cite{wang2022VariationalSymmetricAttack}. If the fluctuations are large enough, they can potentially pose a challenge to cryptography, which makes worst-case guarantees. However, there is no reason to expect that the success probability will scale favorably with key size and compromise security in practice. Another approach is to use amplitude amplification, but adapt it to near-term devices, so that the NISQ-optimized versions perform better in real experiments~\cite{zhang2022NISQSearch}.

\subsubsection*{Outlook}
Here, we focused on the example of symmetric-key encryption. Nonetheless, the effect of amplitude amplification to halve the effective bits of security is generic for computational problems, assuming efficient construction of the oracle. From the cryptographic standpoint, this attack is mild and can be counteracted by doubling the number of bits of security in the scheme. In practice, the increase in key size can be unwieldy in certain applications, such as cryptocurrencies, but fundamental security is not threatened.

\printbibliography[heading=secbib,segment=\therefsegment]
\end{refsection}
