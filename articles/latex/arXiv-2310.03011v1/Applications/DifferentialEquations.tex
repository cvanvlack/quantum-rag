%!TEX root = ../main.tex

\begin{refsection}

\section{Solving differential equations}\label{appl:DiffEq}




\subsubsection*{Overview}



Many applications in engineering and science rely on solving differential equations. Accordingly, this constitutes a large fraction of research-and-development high performance computing (HPC) workloads across a wide variety of industries. Unsurprisingly, there have been many proposals to speed up differential equation solving on a quantum computer. At this point, the consensus is that we lack compelling evidence for practical quantum speedup on industry-relevant problems. However, the field is progressing rapidly and could still provide some surprises. 

Some of the main application areas that have been considered are: 

\begin{itemize}
\item \textbf{Computational fluid dynamics} (CFD), usually involving simulation of the Navier--Stokes equation. The main industries relying on CFD simulations are: automotive, aerospace, civil engineering, wind energy, and defense. While most simulations focus on air or fluid flow on solid objects, other processes, such as foaming, are also important to model. Large CFD calculations are routinely in the petaflop regime and are run on millions of CPU cores. Specific quantum proposals include: \cite{li2023potential,succi2023ensemble,itani2023quantum,joczik2022cost,oz2022solving,gaitan2021finding,gaitan2020finding,chen2022quantum,lapworth2022hybrid}.
\item \textbf{Geophysical modelling}, involving simulation of the wave equation. The main industries are: oil and gas, hydro-electric, geophysics. Large seismic imaging simulations can easily be in the petaflop regime. Quantum proposals for  simulating the wave equation include: \cite{moradi2018quantum,henderson2023quantum,dukalski2021toward}.
\item \textbf{Finite element method} (FEM) for studying structural properties of solid objects. The main industries are: civil engineering, manufacturing (including automotive), aerospace, defense. The simulations are typically slightly smaller in scale than CFD, though still requiring large HPC clusters.  Quantum FEM proposals include: \cite{jin2022time, montanaro2016quantum, van2019quantum,zhang2021quantum}. 
\item \textbf{Maxwell and heat equation} have applications to chip design and other electronic component design, as well as for navigation and radar technology. Specific quantum proposals include: \cite{clader2013preconditioned,jin2022time,linden2022quantum}
\item \textbf{Risk modelling} involving the simulation of stochastic differential equations (SDEs) are extensively used in finance (especially derivatives pricing), insurance, and energy markets. The largest risk modelling simulations can easily be in the petaflop regime, though typically more distributed than CFD calculations. Specific quantum proposals include: \cite{rebentrost2018QuantumFinance,an2021quantum,ramos2021quantum,focardi2020quantum,li2023quantum}. 
\item \textbf{Plasma physics} involving the simulation of the Vlasov equation are widespread in nuclear fusion  research. Quantum approaches include: \cite{novikau2022PlasmaSimulation,engel2019quantum,dodin2021applications}.
\end{itemize}

Differential equations can be categorized according to a number of properties: (a) \textit{ordinary vs.~partial} depending on the number of differential variables, (b) \textit{stochastic vs.~deterministic}, depending on whether the function is a random variable or not, (c) \textit{linear vs.~nonlinear}. We will focus mainly on linear partial differential equations, which constitute the largest class for practical problems, and only comment in passing on  stochastic, or nonlinear differential equations. 

In order to solve a differential equation numerically, one needs to specify a discretization scheme. The two main classes are: (i) \textit{finite difference} and its many variants, including the finite element (FEM) and the finite volume method (FVM) combined with various choices of support grids and preconditioning (see \cite{larson2013finite,trottenberg2000multigrid} for an introduction). In the finite difference framework, the continuous space is discretized on a grid  and the continuous operators are replaced by finite difference operations on neighboring grid points.   Alternatively (ii), one can discretize space by expansion in a functional basis (Fourier, Hermite, etc.), and solve the discretized problem in this space. This second class is often referred to as \textit{spectral methods}. Linear differential equations then map to a linear system of equations. In cases where one is interested in very high precision, requiring very fine discretization, then the linear system of equations can be too large for straightforward numerical solutions on a classical computer. In particular, if one wants high precision results integrated over time, and/or systems with many continuous variables, then the simulations can be challenging both in time and memory. 

\subsubsection*{Actual end-to-end problem(s) solved}

We are interested in solving a general linear partial differential equation of the form 

\begin{equation}\label{eqn:lindiffeq}
    \mathcal{L}(u(x))=f(x)\quad \text{for}\quad x\in \mathbb{C}^d,
\end{equation}
where $\mathcal{L}$ is a linear differential operator acting on the function $u(x)$, and $f(x) \in \mathbb{C}$ specifies the ``geometry'' or some other form of constraint imposed by the particular problem at hand.  
The above form further encompasses ordinary differential equations and initial value problems. As an example, consider the Heat equation in $d+1$ dimensions given by: 
\begin{equation}
    -\frac{\partial u}{\partial t}+\frac{\partial^2 u}{\partial x^2_1}+\cdots + \frac{\partial^2 u}{\partial x^2_d}=0.
\end{equation} 


What does it mean to ``solve'' the differential equation? While closed-form solutions can be derived for some simple differential equations,  this is not possible in general, and the solution typically must be computed numerically. Additionally, in a particular application, we may not need complete information about the function $u(x)$. An end-to-end specification of the problem would be to estimate the value of some property $\mathcal{P}[u] \in \mathbb{R}$ up to specified additive error parameter $\epsilon$.  A straightforward example is when the property $\mathcal{P}$ is simply the value of $u$ at a specific point $x_0$, i.e.~$\mathcal{P}[u] = u(x_0)$. More generally, we restrict to the case where $\mathcal{P}[u]$ is a linear functional of $u$, i.e.~$\mathcal{P}[u] = \langle r, u\rangle := \int_{x \in \Omega} d\Omega \; r(x)u(x)$ for some subset $\Omega \subset \mathbb{R}^d$ and function $r:\Omega \rightarrow \mathbb{R}$ for which $\langle r, r \rangle = 1$ \cite{montanaro2016quantum}. Indeed, in \cite{clader2013preconditioned}, a quantum algorithm for solving Maxwell's equations based on the FEM was given where the quantity of interest was not the electric field itself at any specific point, but rather the electromagnetic scattering cross section. In this case, the cross section was given by the square of a linear functional of $u$.

\subsubsection*{Dominant resource cost/complexity}

For both quantum and classical algorithms, one needs to discretize the continuous degrees of freedom to numerically solve the differential equation. This can take many forms, and the choice of discretization will depend sensitively on the problem at hand. After appropriate discretization,  the linear differential equation in Eq.~\eqref{eqn:lindiffeq} reduces to a matrix equation:

\begin{equation}
    L|u\rangle=|f\rangle.
\end{equation}

From this point on, the linear PDE is typically solved on a quantum computer by applying the \hyperref[prim:QuantumLinearSystemSolvers]{quantum linear system solver} (QLSS), or some variant thereof. The QLSS subroutine prepares a quantum state approximating the solution vector $\ket{u}/\lVert u \rVert$ up to some specified precision $\xi$ in $\ell_2$ norm, where $\lVert u \rVert = \sqrt{\braket{u}{u}}$. Assuming access to oracles that (coherently) query the matrix elements of $L$ and prepare the state $\ket{f}/\lVert f \rVert$, the state-of-the-art QLSS \cite{costa2021OptimalLinearSystem} makes $\bigO{s \kappa \log(1/\xi)}$ queries to these oracles, where $\kappa$ is the condition number of the matrix $L$ (i.e.~the ratio of the largest and smallest singular values), and $s$ is the number of nonzero elements per row of $L$ (``sparsity''). Additionally, learning an estimate for the norm $\lVert u \rVert$ up to multiplicative error $\xi$ can be done in $\bigO{s \kappa / \xi}$ queries (note the worse $\xi$-dependence) \cite{chakraborty2018BlockMatrixPowers}.  
For simplicity, we assume that to achieve $\epsilon$ overall error on the end-to-end problem, it will suffice to take $\xi = \bigO{\epsilon}$, although there can also be factors that depend on the choice of discretization and norms of the solution $u$ (see, e.g., \cite{montanaro2016quantum}). The oracles for querying the matrix elements of the $s$-sparse $N \times N$ matrix $L$ and for preparing the $N$-dimensional state $\ket{f}/\lVert f \rVert$ are assumed to have cost $\mathrm{polylog}(N)$; this is valid if the matrix elements can be efficiently computed ``on the fly,'' or more generally if one has access to a log-depth \hyperref[prim:QRAM]{quantum random access memory};  see \hyperref[prim:LoadingClassicalData]{loading classical data} for more information. 

With these assumptions, the QLSS portion of the quantum algorithm can be performed exponentially faster in $N$, and with exponential savings in memory,  than any classical method that manipulates vectors of size $N$, which includes Gaussian elimination and iterative methods like conjugate gradient. The state-loading assumptions might be very difficult to satisfy in practice, as many practical applications of PDEs involve highly complex geometry in three spatial dimensions (CFD, FEM, seismic modelling).  

Preparing the state $\ket{u}/\lVert u \rVert$ does not immediately yield an estimate for the property $\mathcal{P}[u]$. Indeed, reading out useful information from $\ket{u}/\lVert u \rVert$ represents a major bottleneck of the algorithm. In the case that $\mathcal{P}[u] = u(x_0)$ for a specific point $x_0$, the estimation of $\mathcal{P}[u]$ is performed with \hyperref[prim:AmpEst]{amplitude estimation} (here assuming that the choice of discretization encodes $u(x_0)$ into an amplitude of $\ket{u}$), which introduces multiplicative overhead $\bigO{\lVert u \rVert /\epsilon}$. Note that, to read out all $N$ amplitudes of the state $\ket{u}$ in this fashion, a linear factor of $N$ would be reintroduced, although more advanced methods of \hyperref[prim:Tomography]{pure state tomography} can reduce this to $\sqrt{N}$ \cite{apeldoorn2022TomographyStatePreparationUnitaries}. In the more general case that $\mathcal{P}[u]$ is a linear functional, the value of $\mathcal{P}$ can typically be approximately expressed as an overlap $\braket{\phi}{u}$ between some preparable normalized state $\ket{\phi}$ and the solution vector $\ket{u}$. Overlap estimation is then a straightforward application of \hyperref[prim:AmpEst]{amplitude estimation}, and achieving precision $\epsilon$ introduces $\bigO{\lVert u \rVert /\epsilon}$ multiplicative overhead. Thus, the overall scaling of the complexity is
\begin{equation}
    \frac{s\kappa \lVert u \rVert \log(1/\epsilon) }{\epsilon} \mathrm{polylog}(N).
\end{equation}

The persisting $\mathrm{polylog}(N)$ dependence suggests an exponential speedup in $N$ over classical methods, but this conclusion depends on the scaling of the parameters $s$, $\kappa$, and $\lVert u \rVert$ with $N$. The sparsity $s$ and condition number $\kappa$ depend on the differential equation and the choice of discretization, but can often be controlled as $s=\bigO{1}$ and $\kappa = N^{2/d}$ (e.g.~\cite[Theorem 9.7.1]{brenner2008mathematical}). Additionally, heuristic preconditioning methods are very effective in practice and, in at least one case \cite{clader2013preconditioned}, have been shown to be compatible with the QLSS, which can often reduce the effective value of $\kappa$ to $\bigO{1}$. 
Finally, $N$ and $\epsilon$ are not independent parameters: in general we are interested in simulating a PDE to a fixed precision, and adapt $N$ to reach the desired precision. Using simple grid-based methods, achieving discretization error $\bigO{\epsilon}$ for a problem in $d$ spatial dimensions requires $N = (1/\epsilon)^{\Omega(d)}$, with some caveats on solution norm and continuity \cite{montanaro2016quantum}. Alternative sparse-grid or spectral methods can improve the $1/\epsilon$ dependence to logarithmic, but still scale exponentially with $d$ \cite{childs2021high}. 
A careful analysis of the problem \cite{montanaro2016quantum} shows that for most properties of interest and a fixed precision $\epsilon$, the speedup---irrespective of the discretization scheme---from QLSS is at best polynomial in $1/\epsilon$, even assuming good control over the condition number $\kappa$. Indeed, when we assume $s,\kappa = \bigO{1}$, in addition to the assumptions from above, the quantum complexity is
\begin{equation}
    \bigOt{\lVert u \rVert/\epsilon}\,.
\end{equation}

Ultimately, this observation is traced back to the fact that the quantum solver produces a quantum state encoding the solution to the PDE, potentially exponentially faster than leading classical methods, such as conjugate gradient, but the exponential speedup is lost in the readout step. Moreover, this conclusion holds not just for ``bad'' observables (like full state tomography), but for any observable, due to the $\Omega(1/\epsilon)$ cost of quantum readout.\\


A distinct approach to solving PDEs on a quantum computer is based on mapping the PDE directly to the Schr\"odinger equation and performing the time evolution on a quantum computer (via \hyperref[prim:HamiltonianSimulation]{Hamiltonian simulation}) \cite{fang2022time, babbush2023exponential}. This is also the typical approach employed for solving nonlinear differential equations \cite{leyton2008quantum,lloyd2020quantum,liu2021dissipativeNonlinearEqs,an2022efficient,krovi2023improved}. In this approach, with appropriate assumptions on the initial state or boundary condition encoding, it is once again possible to obtain an exponential speedup in the time to prepare the time evolved state $|u(T)\rangle$. Moreover, this approach may avoid the condition number dependence that results from matrix inversion in the \hyperref[prim:QuantumLinearSystemSolvers]{QLSS} approach. Nevertheless, the same conclusions discussed above regarding readout still hold, restricting the quantum algorithm to a polynomial speedup for the practical task of measuring observables with respect to the solution of the differential equation (for constant dimension $d$).


Finally, we comment on two further classes of applications involving PDEs, but which typically have very different characteristics: The first is stochastic differential equations (SDEs) which are simulated extensively in \hyperref[appl:finance]{computational finance}  and more generally in risk modeling. There, one typically samples trajectories of the SDE (via Monte Carlo methods), and evaluates observables stochastically. Quantum accelerated Monte Carlo has been worked on extensively (see the \hyperref[appl:OptionsPricing]{options pricing} page). However, these algorithms involve very different tools from the ones discussed here. Alternatively, one could map a SDE to a Fokker--Planck equation via the It\^o calculus and solve the Fokker--Planck PDE. This has been proposed in \cite{gonzalez2021simulating}.  However, for most SDEs of interest in risk analysis, Monte Carlo simulation converges in a number of samples scaling linearly in the  number of variables, leaving very little room for a quantum speedup in these applications given our current understanding.



The last class of problems to be mentioned are multi-particle Schr\"odinger equations. They are (a) high dimensional, (b) complex, and (c) require high precision solutions for practical applications. Hence matching all of the criteria under which a quantum advantage might be expected. The second quantized approach to solving the full configuration interaction molecular   Schr\"odinger equation is a specific case of the spectral method. Unsurprisingly, this case has already gathered a lot of attention (see the application section \hyperref[appl:QuantumChemistry]{quantum chemistry}).






\subsubsection*{Existing error corrected resource estimates}
There do not exist many such resource estimates so far, though they should follow from similar estimates for the \hyperref[prim:QuantumLinearSystemSolvers]{quantum linear system solver}. See \cite{costa2021OptimalLinearSystem, jennings2023QLSS} for a state-of-the-art analysis. 
Note, however, that much of the art in classical PDE solvers is to find appropriate preconditioning schemes to control the condition number. In \cite{clader2013preconditioned}, it was shown that one common class of preconditioners works within the framework of the quantum algorithm, but it is as of yet unclear if this is the case more generally. 

One explicit resource estimate for the end-to-end problem discussed above was given in \cite{scherer2017concrete}, which estimated that to beat the best classical solvers: ``a desired calculation accuracy $0.01$ requires an approximate circuit width $340$ and circuit depth of order $10^{25}$ if oracle costs are excluded, and a circuit width and depth of order $10^8$ and $10^{29}$ if oracle costs are included.'' 
These estimates are not very encouraging, yet many orders of magnitude can be shaven off of them with more recent synthesis and simulation methods. We expect that using the state-of-the-art \hyperref[prim:QuantumLinearSystemSolvers]{quantum linear system solver} 
the Tofolli gate count can be brought down by orders of magnitude, likely in the vicinity of $10^{11-15}$ depending upon the specific setting.  
Reliable estimates would require a more careful study. 


\subsubsection*{Caveats}


An essential caveat is that the speedup for solving PDEs largely depends on what speedup one might obtain from the QLSS algorithm. This is very contentious, as quantum-inspired methods \cite{tang2018QInspiredClassAlgPCA} are getting dangerously close to the same asymptotic scaling as the QLSS in many specific instances (see \hyperref[appl:ClassicalML]{quantum machine learning} section). Since the QLSS depends sensitively on the condition number, very careful analysis of the preconditioning scheme must be made on a case-by-case basis. Even if quantum-inspired methods cannot compete, classical iterative methods such as conjugate gradient, which also benefit from a small condition number, can be difficult to beat, especially when they are running on high performance computing hardware with many parallel CPUs.



Furthermore, incorporating highly complex geometry or boundary conditions into the problem, as is often necessary for CFD and finite element computations, might constitute a major \hyperref[prim:LoadingClassicalData]{state-loading} bottleneck.
Finally, the observables of interest in classical PDE problems might require near \hyperref[prim:Tomography]{full tomography} of the quantum solution state which in certain situations removes all quantum advantage \cite{linden2022quantum}. 

In instances where these caveats can be overcome, the speedups are at best polynomial, with a larger speedup in a larger number of spatial dimensions. However, in many engineering applications, the number of dimensions is fixed to be fewer than four (three for space, one for time), limiting the advantage quantum methods can obtain. 

Note that another approach for time evolving PDE based on time-stepping has been proposed recently, which might provide more advantage \cite{fang2022time} (see also \cite{babbush2023exponential} for a query based model with exponential speedup in certain specific variables). 


\subsubsection{Comparable classical complexity and challenging instance sizes}

While the size and scale of PDE simulations vary widely, some of the largest and most costly are CFD simulations. CFD computations on a 3D grid with several billion mesh nodes (in finite volume discretization) running on tens of thousands of CPU cores are routine \cite{lapworth2022hybrid}. It is unclear what the main bottleneck for a quantum simulation of such a large system will be; the condition number and preconditioning, loading the geometry, or readout of the result? In any case, this scale seems well out of reach of the current fault-tolerant algorithmic projections. 



\subsubsection*{Speedup}

In this section, we consider the complexity of either estimating a function of the PDE solution, or of outputting quantum states encoding the solution, to precision $\epsilon$ for a system in $d$ spatial dimensions.  Outputting a quantum state encoding the PDE solution will typically have a much more favorable quantum complexity, though it is not necessarily a fair comparison, as much of the speedup can be lost at readout.  

\textit{Finite difference methods.} Finite element, finite volume methods and their variants are the leading class of methods used in industry. These methods typically have a complexity/dimension scaling of ${\rm poly}(\epsilon^{-d})$. In \cite{montanaro2016quantum} a quantum algorithm for the homogenous Poisson equation in $d$ dimensions, with homogenous boundary conditions is found to have a complexity scaling as ${\rm poly}(d,1/\epsilon)$.
In \cite{fillion2019simple}, a quantum algorithm for outputting the solution of a hyperbolic PDE  in the finite volume discretization was proposed with a $d \cdot {\rm poly}(\epsilon^{-1})$ scaling. 

\textit{Spectral methods.} References \cite{childs2020quantum,childs2021high} explore the spectral method for ODEs and PDEs. Their quantum algorithm for outputting the solution of an elliptic PDE with Dirichlet boundary conditions scales as $d^2 \cdot {\rm polylog}(\epsilon^{-1})$ (note that an additional factor of $1/\epsilon$ for readout would be incurred to solve the fully end-to-end problem), compared to general classical spectral methods scaling as ${\rm poly}(\epsilon^{-d})$ \cite{shen2011spectral}.\\

The quoted scalings need to be considered with caution. Firstly, the quantum algorithms are not fully end-to-end but rather just output the quantum state representing the solution. Whether relevant functions (observables) of the solutions can be extracted efficiently from the quantum state will depend on the specific task at hand. Secondly, the condition number and the associated preconditioning scheme of the linear system under consideration is neither quoted in the classical  nor in the quantum scalings. This could be the dominant cost of the algorithm in real settings. Thirdly, the scalings do not take state/geometry loading into account. This point is particularly sensitive, as industry-relevant problems often need just as high precision in geometry specification, as in solution quality. Capturing high precision geometry is often more challenging with spectral methods than with finite difference methods.  

Nevertheless, the take-home message  is that quantum algorithms can potentially outperform classical algorithms, but major gains are only to be expected when the number of  dimensions is  large. This intuition is corroborated by the analysis of quantum computing algorithms for \hyperref[appl:QuantumChemistry]{ab initio chemistry}, where the number of dimensions scales with the number of electrons. Substantial memory savings also seem likely in this setting. 




\subsubsection*{NISQ implementations}

Various proposals at NISQ implementations of PDE solvers have been made; see \cite{leong2022variational} and references therein. The idea is to start from some discretization of the PDE $L|\psi(\theta)\rangle = |b\rangle$, where $|\psi(\theta)\rangle$ is an appropriately chosen variational circuit, and optimize the parameters of the circuit. This is an example of a \hyperref[prim:VQA]{variational quantum algorithm.} It is difficult to imagine that sufficient size and precision can be reached in the NISQ regime to be competitive with the best classical solvers. 


\subsubsection*{Outlook}
While the simulation of PDEs is one of the most important large-scale computational tasks, constituting a sizable fraction of HPC workloads in industry, at present the benefit of quantum solvers is still too limited in dimensions up to four. To find a killer application of quantum algorithms for PDEs (beyond ab initio chemistry), one would need to find an important application of high dimensional PDEs, requiring very high precision solutions while involving relatively simple geometry or initial conditions, and that can't be solved accurately with any classical methods at present. There remains the possibility for substantial improvements in memory usage, but these are not currently a bottleneck in classical PDE solving. 
Recent progress \cite{fang2022time,babbush2023exponential} suggests that in very specific scenarios, there might still be room for substantial gains on quantum hardware, but it is as yet unclear how practical or relevant these scenarios are. 




\printbibliography[heading=secbib,segment=\therefsegment]
\end{refsection}

\newpage