%!TEX root = ../main.tex

\begin{refsection}

\section{Finance }\label{appl:finance}

While several industries stand to benefit from quantum computing, the financial services industry has historically been an early adopter of quantum technology by investing in research and development efforts in the area of quantum finance. Finance has the distinct feature that more powerful and more accurate simulations can lead to direct competitive advantage, in a way that is harder to identify in other industries. In this application area, researchers strive to find quantum speedups for use cases of interest to financial services. A number of use cases have been proposed as candidates for quantum solutions, such as:
\begin{itemize}
    \item \textbf{Derivative pricing} (such as options~\cite{stamatopoulos2020option}, and collateralized debt obligations (CDO)~\cite{tang2021quantum}). Derivatives are financial instruments that are built upon an underlying asset (or assets) that can depend on the value of the asset in potentially complicated ways. In the derivative pricing problem, one needs to determine a fair price of the financial instrument, which typically depends on an expected value of the underlying assets at some later date. A similar and related problem is known as \textbf{computing the Greeks}~\cite{stamatopoulos2022towards}. The Greeks of a financial derivative are quantities that determine the sensitivity of the derivative to various parameters in the problem. For example, the Greeks of an option are given by the derivative of the value of the option with respect to some parameter, e.g., $\Delta:=\partial V/\partial X$, where $V$ is the value of the option and $X$ is the price of the underlying asset. 
    \item \textbf{Credit valuation adjustments (CVA)}~\cite{han2022quantum}. CVA is the problem of determining the fair price of a derivative, portfolio, or other financial instrument that is extended to a purchaser on credit, and that takes into account the purchaser's (potentially poor) credit rating, and the risk of default. CVA is typically given by the difference between the risk-free portfolio and the value of the portfolio taking into account the possibility of default.
    \item \textbf{Value at risk (VaR)}~\cite{woerner2019quantum}. Many forms of risk analysis can be considered, with VaR being a common example. VaR measures the total value a financial instrument (such as a portfolio) might lose over a predefined time interval within a fixed confidence interval. For example, the VaR of a portfolio might indicate that, with 95\% probability, the portfolio will not lose more than $\$Y$. A similar technique works as well for the related Credit Value at Risk (CVaR) problem.
    \item \textbf{Portfolio optimization}~\cite{rebentrost2018QuantumFinance}. The goal of portfolio optimization is to determine the optimal allocation of funds into a universe of investable assets such that the resulting portfolio maximizes returns and minimizes risk, while also respecting other constraints.
\end{itemize}
While there are are many more use cases and several approaches for generating quantum speedups, broadly speaking, many uses cases stem from one of two paths to quantum improvements: quantum enhancements to Monte Carlo methods (for simulating stochastic processes), and constrained optimization. 
In the first case, the approach generally involves encoding a relevant, problem-specific function into a quantum state, and then using \hyperref[prim:AA]{quantum amplitude estimation} to sample from the distribution quadratically fewer times than classical Monte Carlo methods~\cite{montanaro2015QMonteCarlo}. In the second case, a financial use case is reduced to a constrained optimization problem, and a quantum algorithm for \hyperref[appl:ContinuousOpt]{optimization} is used to solve the problem. 

Among the use cases studied in these two areas, option pricing and portfolio optimization often serve as archetypal examples of Monte Carlo and constrained optimization problems, respectively, and their associated quantum algorithms have the most follow-up work. Moreover, these two classes of problems comprise a considerable fraction of the classical compute used in the financial services industry. For these reasons, we will focus on these two use cases in this section, though the approaches, caveats, and complexities can (usually) be readily carried over to other relevant use cases.

In addition to the use cases described above, other areas of interest to the financial services industry include post-quantum cryptography, quantum-secure networking and quantum key distribution, etc. However, many of these topics or their proposed quantum implementations are outside the scope of this document. \hyperref[appl:ClassicalML]{Quantum machine learning} is yet another popular use case within quantum finance, but oftentimes these results are quantum approaches to standard machine learning problems, which are then applied to a financial application. As such, we will also not study machine learning in this finance-specific section, and we instead refer interested readers to any of the excellent review articles on quantum finance (e.g.,~\cite{herman2022survey, bouland2020prospects}) for more details.

\localtableofcontents
%%
\printbibliography[heading=secbib,segment=\therefsegment]
\end{refsection}
\newpage

\begin{refsection}
\subsection{Portfolio optimization}\label{appl:PortfolioOptimization}
\subsubsection*{Overview}
Given a set of possible assets into which one can invest, the problem of portfolio optimization (PO) involves finding the optimal allocation of funds into these assets so as to maximize returns while minimizing risk.
The Markowitz model, as it is commonly called, is widely used in the financial industry, owing to its simplicity and broad applicability. Sophisticated constraints, transaction cost functions, and modifications to the problem can be used to model realistic, modern portfolio optimization problems. Numerically solving these optimization problems is a routine part of existing workflows in financial services operations.  Several quantum approaches to solving the portfolio optimization problem have been proposed, each with their own advantages and drawbacks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Actual end-to-end problem(s) solved}

Consider a set of $n$ investable assets with a fixed total budget. Define $w_i \in \mathbb{R}$ to be the fraction of the total budget that is invested into asset $i$. Thus, the $n$-dimensional vector $w$ defines a portfolio. Let $r$ be a known $n$-dimensional vector denoting the expected return for each of the available assets, i.e.~the percentage by which the value of each asset is expected to grow over some defined time period. Let $\Sigma\in\mathbb{R}^{n\times n}$ be the covariance matrix governing the random (and possibly correlated) fluctuations in the asset returns away from their mean $r$.
In practice, the input parameters $\Sigma$ and $r$ can be inferred from historical stock price data, or through more sophisticated analyses.
The covariance matrix can be used to define a portfolio's ``risk'' $w^\intercal \Sigma w$, which is precisely the variance in the returns it generates, assuming the underlying model is accurate. Denote the all-ones vector by $\OnesVec$, and for any pair of vectors $u,v$ let $\langle u,v\rangle$ denote the standard inner product between $u$ and $v$. The goal of the Markowitz formulation of portfolio optimization is to find the optimal portfolio (i.e., vector of weights $w$) that either:
\begin{itemize}
    \item maximizes the expected return subject to a fixed risk parameter $\sigma_0^2$
    \begin{equation}
    \begin{split}
     \max_{w}\; &\langle w,r\rangle \\
    \mathrm{s.t.}\quad w^\intercal \Sigma w&=\sigma_0^2\\
    \quad \langle \OnesVec, w\rangle &=1
    \end{split}
    \end{equation}
    \item minimizes risk subject to a fixed return parameter $r_0$
    \begin{equation}
    \begin{split}\label{eq:PO_fixed_return}
    \min_{w}\;  &w^\intercal \Sigma w\\
    \mathrm{s.t.}\quad \langle w,r\rangle &=r_0\\
    \langle \OnesVec, w\rangle &=1
    \end{split}
    \end{equation}
    \item maximizes return and minimizes risk with a tradeoff determined by a parameter known as the ``risk aversion parameter'' $\lambda$:
    \begin{equation}\label{eq:quadratic_objective}
    \begin{split}
    \max_{w}\; \langle w,r \rangle- \lambda&w^\intercal \Sigma w\\
    \mathrm{s.t.}\quad \langle\OnesVec, w\rangle &=1
    \end{split}
    \end{equation}
    or the alternative for the square root of risk (standard deviation rather than variance)
    \begin{equation}
    \begin{split}
    \max_{w}\; \langle w,r\rangle- q&\sqrt{w^\intercal \Sigma w}\\
    \mathrm{s.t.}\quad \langle\OnesVec, w\rangle &=1,
    \end{split}
    \end{equation}
    where $q$ plays the same role as $\lambda$.
\end{itemize}
Typically, it is satisfactory to find a vector that optimizes the objective function up to additive error $\epsilon$, for some prespecified value of $\epsilon$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

When solving the above Markowitz model formulations of PO, the absence of inequality constraints leads to simpler optimization problems that can be solved with simpler approaches. For example, the optimization problem in Eq.~\eqref{eq:PO_fixed_return} is a simple quadratic program without complicated constraints, for which one can derive a closed-form expression for $w$ using Lagrange multipliers \cite{merton1972analytic}. More general portfolio optimization problems that include practically relevant constraints (such as the simple ``long-only'' constraint $w_i\geq 0$, which contrasts ``short'' positions in which $w_i$ can be less than zero) cannot generically be solved analytically, and one needs to employ more sophisticated numerical solvers.
Real-world portfolio optimization problems include a number of possible constraints (see \cite{MosekPortfolio2021} for a discussion), including, but not limited to:
\begin{itemize}
    \item Long only --- $w_j\geq 0, \quad \forall j$
    \item Investment bands --- $w_j\in [w_j^\text{min}, w_j^\text{max}]$
    \item Turnover constraints --- $|\Delta w_j|\leq U_j$ for some fixed fraction $U_j$, where $\Delta w_j$ represents the change in holdings of asset $w_j$ from one portfolio to the next.
    \item Cardinality constraints --- minimum, maximum, or exact number of nonzero assets in the portfolio
    \item Sector constraints --- specified minimum and/or maximum allocations to groups of assets (e.g., the energy or healthcare sectors)
    \item Transaction costs --- typically represented as a function of $|\Delta w_j|$, and often added as a term in the objective function rather than as a constraint
\end{itemize}
As is often the case with optimization problems, the problem formulation \textit{strongly} affects the solution strategy and the problem ``hardness.'' If the PO problem is unconstrained and continuous (i.e., each $w_i$ is a real number), then the problem is relatively easy. If convex inequality constraints, such as the long-only  or turnover constraints, are imposed, the problem is harder, but can still be tackled by relatively efficient methods for \hyperref[appl:ContinuousOpt]{convex optimization}. By contrast, if one discretizes the problem (so that $w$ now represents an integer number of asset shares or lots being traded), or if one applies some of the constraints above (such as integer-valued constraints like cardinality), then the problem becomes nonconvex and considerably harder to solve. In general, with discrete constraints, the problem can be formulated as an instance of mixed-integer program (MIP), which is NP-complete and therefore intractable to solve in polynomial-time (in $n$) under widely believed assumptions. Alternatively, by encoding the integer variables in binary, it can be formulated as a quadratic unconstrained binary optimization (QUBO) instance. These formulations allow quantum algorithms for \hyperref[appl:CombOpt]{combinatorial optimization} to be employed; for example, the MIP formulation can be solved with a branch-and-bound approach~\cite{chakrabarti2022universal}, and the QUBO formulation can be solved via \hyperref[appl:SearchAlgorithms]{Grover-type methods}, or heuristically through (NISQ-friendly) \hyperref[prim:QuantumAdiabaticAlgorithm]{quantum annealing} approaches (e.g.,~\cite{mugel2022dynamic}). 


\subsubsection*{Dominant resource cost/complexity}

An early approach to solving this optimization problem using a quantum algorithm was presented in~\cite{rebentrost2018QuantumFinance}, in which the Markowitz problem is written as minimizing risk with fixed return (Eq.~\eqref{eq:PO_fixed_return}), and without other complicated constraints. This simple optimization problem boils down to an equality constrained convex program; it can be solved by introducing Lagrange multipliers and solving a linear system involving the input data $r$ and $\Sigma$ \cite{rebentrost2018QuantumFinance}. The approach of \cite{rebentrost2018QuantumFinance} is to use a  \hyperref[prim:QuantumLinearSystemSolvers]{quantum linear system solver} (QLSS) and prepare the quantum state $\ket{w}$ whose amplitudes are proportional to the optimal weights $w_i$. The complexity to do so to error $\epsilon$ is $\bigOt{\kappa\zeta\log(1/\epsilon)}$, where $\kappa$ is the condition number of the matrix $G$ being inverted and $\zeta = \lVert G \rVert_F/\lVert G \rVert$ is the ratio of its Frobenius norm to its spectral norm. The $\tilde{\mathcal{O}}$ suppresses logarithmic factors, including a factor coming from applying unitaries that \hyperref[prim:BlockEncodingsClassical]{block-encode} the matrix $G$ in $\mathrm{polylog}(n)$ depth, essentially equivalent to the assumption that log-depth \hyperref[prim:QRAM]{quantum random access memory} (QRAM) is available. It is a priori unclear what the value of $\kappa$ and $\zeta$ would be for actual PO instances and whether they depend on $n$, but the explicit logarithmic dependence of this complexity on $n$ is appealing. 
However, a drawback of this approach is that it produces the quantum state $\ket{w}$ rather than an estimate for the optimal portfolio $w$. Learning the $n$ entries of $w$ to precision $\epsilon$ in 2-norm incurs multiplicative overhead of $\bigOt{n/\epsilon}$ using quantum pure-state \hyperref[prim:Tomography]{tomography} \cite{apeldoorn2022TomographyStatePreparationUnitaries} for total time complexity $\bigOt{n\kappa\zeta/\epsilon}$.\footnote{Reference~\cite{rebentrost2018QuantumFinance} suggests several possible nonstandard problems that can be solved with $\ket{w}$ without actually learning the entries of $w$, such as sampling values of $i$ with large $|w_i|$, and estimating overlaps $\langle \tilde{w}, w \rangle$ with hypothesized portfolios $\tilde{w}$. In general, inner products $\langle u, w \rangle$ of arbitrary normalized vectors $u$ with $w$ can be learned to precision $\epsilon$ using overlap estimation \cite{knill2007ObservableMeasurement} (an application of \hyperref[prim:AmpAmp]{amplitude estimation}), incurring multiplicative overhead of $\bigO{1/\epsilon}$, but no explicit linear-in-$n$ dependence. However, the practical utility of such tasks within the existing workflows of financial institutions is unclear.} 


When convex linear inequality constraints, such as long-only or turnover constraints, are included, the above approach will not work. However, a more sophisticated method can be applied, which first maps the PO instance to a \hyperref[appl:ConicProgramming]{convex program} (specifically, a second-order cone program (SOCP)) and then makes use of interior point methods to solve the program. These interior point methods can be quantized, forming \hyperref[prim:QIPM]{quantum interior point methods} (QIPMs) \cite{kerenidis2019QAlgsSecondOrderConeSVM,augustino2022inexact}.
The QIPM is an iterative method, where each iteration involves solving a linear equation with a \hyperref[prim:QuantumLinearSystemSolvers]{QLSS} and classically reading out the solution with \hyperref[prim:Tomography]{tomography}. Thus, the procedure within each iteration is similar to the procedure above for solving the unconstrained PO problem, but the linear system to be solved is different (and changes with each iteration). A preliminary study of the effectiveness of this approach for PO was given by~\cite{kerenidis2019PortfolioOptimization}, and a more extensive study later appeared in \cite{dalzell2022socp}. The QIPM produces an $\epsilon$-optimal classical estimate for $w$, and has time complexity $\tilde{\mathcal{O}}(n^{1.5}\frac{\zeta\kappa}{\xi}\log(1/\epsilon))$, where $\kappa$ and $\zeta$ are the maximum condition number and Frobenius-to-spectral-norm ratio for the matrices that must be inverted over the course of the algorithm, respectively, and  $\xi$ is the precision to which tomography must be performed. Note that in principle $\xi$ can stay constant even as the overall precision estimate $\epsilon \rightarrow 0$ \cite{dalzell2022socp}.  

With the addition of discrete constraints, PO is instead formulated as a nonconvex MIP. MIPs are typically solved with a branch-and-bound approach (for a summary in a financial context, see, e.g., \cite[Chapter 11]{cornuejols2006optimizationFinance}). Key to this approach is the ability to solve convex relaxations of the MIP where the discrete constraints are dropped in $\mathrm{poly}(n)$ time (perhaps via classical or quantum interior point methods for SOCPs, as above). To impose the discrete constraints, a tree is constructed and explored, where generating the children of a given node in the tree requires solving one of these relaxations. Thus, the number of convex relaxations that must be solved is proportional to the tree size $T$, which is generally exponentially large in $n$. Reference~\cite{chakrabarti2022universal} (extending prior work of \cite{montanaro2019QBranchAndBound}) showed that a quantum algorithm can produce the same output while exploring quadratically fewer nodes, solving roughly $\tilde{\mathcal{O}}(\sqrt{T})$ convex relaxations (but doing so coherently, which could introduce overheads), for a total complexity of $\tilde{\mathcal{O}}(\sqrt{T}) \cdot \mathrm{poly}(n)$. The value of $T$ is instance dependent and requires empirical estimation: a preliminary numerical analysis of the value of $T$ for a certain ensemble of PO instances up to $n=56$ found that $T\sim 2^{0.14n}$ to $2^{0.20n}$ \cite{chakrabarti2022universal}.

The assessment of the number of qubits used by these algorithms requires a nuanced discussion of \hyperref[prim:LoadingClassicalData]{data loading}. A key feature of all of the approaches above is that they require (repeatedly) accessing the classical data representing the historical stock information (i.e., the returns $
r$ and the covariance matrix $\Sigma$) into the quantum algorithm. The size of this data is typically $\bigO{n^2}$.
Loading can be performed using \hyperref[prim:BlockEncodings]{block-encodings} and \hyperref[prim:QRAM]{QRAM}, which achieves $\bigO{\log(n)}$ depth (time), at the expense of $\bigO{n^2}$ space. Here, several caveats are inherited from the \hyperref[prim:QRAM]{QRAM} primitive. Moreover, for practical values of $n$, this $\bigO{n^2}$ space cost could be prohibitively large, although it is possible this space cost could manifest as a dedicated QRAM hardware element of the device, rather than as part of the main processor. If log-depth QRAM of sufficient size is not desired or not available, the data could instead be loaded with only $\bigO{\log(n)}$ space and in $\bigO{n^2}$ time, but this overhead in time would likely preclude the possibility of quantum speedup at least in the first two cases, where the formulation is convex and classical $\mathrm{poly}(n)$-time algorithms exist. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Existing error corrected resource estimates}

A detailed, end-to-end resource analysis of the PO problem using QIPMs was performed in \cite{dalzell2022socp}. The authors followed the approach of \cite{kerenidis2019PortfolioOptimization} and performed a careful accounting of all quantum resources, including constant prefactors. The authors found that one needs $800n^2$ logical qubits, a $T$-depth of
\begin{equation*}
    (2\times 10^8)\kappa\zeta n^{1.5}\xi^{-2}\log_2(n)\log_2(\epsilon^{-1})\log_2(\kappa\zeta n^{14/27}\xi^{-1}),
\end{equation*}
and a $T$-count of
\begin{equation*}
    (7\times 10^{11})\kappa\zeta n^{3.5}\xi^{-2}\log_2(n)\log_2(\epsilon^{-1})\log_2(\kappa\zeta \xi^{-1}),
\end{equation*}
where $\kappa$ is the maximum condition number encountered in the algorithm, $\zeta$ is the maximum Frobenius-to-spectral-norm ratio, and $\xi$ is the minimum tomographic precision required. The $\xi^{-2}$ dependence can asymptotically be improved to $\xi^{-1}$ at the expense of a more sophisticated protocol for \hyperref[prim:Tomography]{tomography} \cite{apeldoorn2022TomographyStatePreparationUnitaries}. Note also that this calculation incorporated optimized circuits for \hyperref[prim:BlockEncodingsClassical]{block-encoding} with $\bigO{\log(n)}$ $T$-depth but $\bigO{n^2}$  $T$-count \cite{clader2022resourcesForBlockEncoding}, leading to the large discrepancy between those two quantities. The authors performed numerical simulations of portfolio optimization instances to determine the instance-specific quantities. Using numerically determined values for $\kappa\zeta$ and $\xi$, and using realistic values of $\epsilon=10^{-7}$ and $n=100$, these resource counts imply that one would need $8\times 10^6$ logical qubits, $2\times 10^{24}$ $T$-depth, and $8\times 10^{29}$ $T$-count. These logical estimates for the number of non-Clifford gates could in principle be turned into estimates for the number of physical qubits and runtime on actual hardware, using the methods discussed in the \hyperref[prim:FTQC]{page on fault-tolerant quantum computation}. However, the authors of \cite{dalzell2022socp} did not do so, in part because the logical costs were sufficiently high that the qualitative conclusion about the practicality of the algorithm was already clear. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Caveats}

The quantum algorithms for PO discussed above inherit many of the caveats of their underlying primitives, namely \hyperref[prim:QuantumLinearSystemSolvers]{QLSS}, \hyperref[prim:Tomography]{tomography}, and \hyperref[prim:LoadingClassicalData]{classical data loading}. One salient caveat is that the QLSS-based approaches depend on a number of instance-specific parameters $\kappa, \zeta,\xi$, which are difficult to predict without running numerical simulations. The asymptotic speedup is subject to assumptions about the scaling of these parameters. Additionally, for a speedup to be possible, log-depth \hyperref[prim:QRAM]{QRAM} must be available on large datasets, which, while theoretically possible, presents practical challenges. 

The branch-and-bound approach does \textit{not} require log-depth QRAM to achieve its nearly quadratic speedup since the runtime will be dominated by the exponential tree-size factor (although it would help to have fast QRAM to reduce by $\mathrm{poly}(n)$ factors the time needed to solve the convex relaxations at each step). However, a caveat to that approach is that to obtain the quadratic speedup, the convex relaxations of the MIP (which would be SOCPs), would need to be solved coherently. In principle, this is always possible, but it would likely require a substantial amount of coherent classical arithmetic and additional $\mathrm{poly}(n)$ overheads in time and space. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Comparable classical complexity and challenging instance sizes}

Convex formulations of the PO problem are typically solved classically via mapping to SOCP. Optimized software packages can solve these SOCPs efficiently, and many are based on interior point methods. These interior point methods have theoretical runtime complexity of roughly $\tilde{O}(n^{\omega+0.5}\log(1/\epsilon))$, where $\omega\approx 2.373$ is the matrix multiplication exponent, although for practical instance sizes, the effective value of $\omega$ is typically closer to 3. Note that the example PO problem with 100 assets solved in \cite{dalzell2022socp} and described above can typically be solved within seconds on a laptop using traditional classical methods. Problem sizes found in the financial services industry can include as many as tens of thousands of assets.

In the MIP formulation of PO, classical solutions will have complexity exponential in $n$. As a point of reference, the numerical experiments reported in \cite{chakrabarti2022universal} classically solved hundreds of PO instances up to size $n=56$ (and likely could have gone significantly higher).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Speedup}


Recall that the QIPMs used to solve the SOCP for constrained PO are virtually identical to their classical counterpart; they differ by their use of a quantum subroutine to solve linear systems. Thus, any speedup obtained by the quantum approach to solving the SOCP will necessarily come from speedups from the \hyperref[prim:QuantumLinearSystemSolvers]{QLSS} plus \hyperref[prim:Tomography]{tomography} approach to solving a linear system. The approach for unconstrained PO was also based on the same primitives. 
The performance of the quantum method is often compared against classical  Gaussian elimination. However, since the quantum approach necessarily produces an approximate solver (due to tomography), another valid comparison to make is against approximate classical solvers, such as the randomized Kaczmarz method~\cite{strohmer2009kaczmarz}. In this case, the classical complexity for solving an $L\times L$ linear system to precision $\xi$ scales as $\mathcal{O}(L\kappa^2 \zeta^2\log(\xi^{-1}))$ (where $\kappa$ is the condition number and $\zeta$ the Frobenius-to-spectral norm ratio) compared to $\mathcal{O}(L^3)$ for Gaussian elimination (asymptotically $\mathcal{O}(L^{\omega})$). Thus, the quantum method provides the greatest speedup when $\kappa\zeta\propto L$ and $\xi=\bigO{1}$, in which case the QIPM for constrained PO runtime scales as $\bigOt{n^{2.5}}$, whereas the classical runtimes scales as $\bigOt{n^{3.5}}$, where $n$ is the number of stocks in the portfolio (see \cite[Table XI]{dalzell2022socp} for a more complete discussion). For unconstrained PO, which only requires solving one linear system, the comparison would be $\bigOt{n^2}$ vs.~$\bigOt{n^3}$. In either case, the speedup is subquadratic. Moreover, the numerical simulations in \cite{dalzell2022socp} were not consistent with these optimistic assumptions on $\kappa \zeta$ and $\xi$, suggesting, rather, that the QIPM would have minimal if any speedup over classical IPMs, albeit based on small instance sizes up to $n=120$. 

The speedup for the branch-and-bound approach to the MIP formulation of PO is quadratic (up to log factors), although, as mentioned, in contrast to the convex formulations, both the quantum and classical algorithms generally have runtime exponential in $n$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{NISQ implementations}
Several alternative approaches to portfolio optimization using quantum solutions have been proposed.
\begin{itemize}
    \item NISQ-HHL~\cite{yalovetzky2021nisqHHL}. This work generalizes the algorithm of \cite{rebentrost2018QuantumFinance}, described above, by employing mid-circuit measurements and conditional logic to obtain a NISQ version of the \hyperref[prim:QuantumLinearSystemSolvers]{QLSS} that readily solves the PO problem.
    \item \hyperref[prim:VQA]{QAOA} approaches: 
\cite{brandhofer2022benchmarking,herman2022portfolio,baker2022wasserstein}
    . These approaches typically use the quadratic objective function from Eq.~\eqref{eq:quadratic_objective}, but instead consider $w_i \in \{0,1\}$ as binary variables indicating whether or not an asset is part of the portfolio (a substantial deviation from the normal formulation). Constraints are dealt with by adding penalties to the objective function.  Alternatively, constraints are enforced by choosing clever versions of the ansatz \cite{niroula2022constrained} or by making measurements to project into the feasible space \cite{herman2022portfolio}.
    \item \hyperref[prim:QuantumAdiabaticAlgorithm]{Quantum annealing} approaches: \cite{rosenberg2016tradingQuantumAnnealing,palmer2022financial,palmer2021POwithBands,grant2021benchmarking,mugel2022dynamic}. As in the previous case, these approaches require the problem to be formulated as a binary optimization problem. However, in this case, they typically take the MIP formulation and encode integers in binary through one of several possible encodings \cite{rosenberg2016tradingQuantumAnnealing} (thus, the number of binary variables will be greater than $n$).   Constraints in the PO problem can also be included in the objective function using a variety of tricks, resulting in the desired QUBO, which can then be solved using a quantum annealer.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Outlook}

The QIPM approach (and QLSS-based techniques more generally) for continuous formulations of PO have the potential to offer polynomial (but subquadratic) speedups for the PO problem.  However, these speedups are subject to conjectures about the scaling of certain instance-specific parameters and preliminary empirical estimates are not suggestive of a maximal speedup. In any regard, the resource estimates of~\cite{dalzell2022socp} illustrate that the non-Clifford resources required to implement the QIPM for this use case are prohibitive, even at problem sizes that are trivial to solve with classical computers. An asymptotic quantum advantage for this problem could exist for sufficiently large sets of assets, but without drastic improvements to the quantum algorithm and the underlying primitives (e.g., QRAM, QLSS), it is unlikely this approach will be fruitful. Even if such improvements are made, the algorithm only provides a polynomial speedup that is subquadratic, at best, greatly limiting the upside potential of this approach. 

The branch-and-bound approach for discrete formulations has a possible for a larger quadratic speedup, but, as has been observed (see, e.g., \cite{campbell2019ApplyingQToCSPs,babbush2021FocusBeyondQuadratic}) in the context of Grover-like quadratic speedups in \hyperref[appl:SearchAlgorithms]{combinatorial optimization}, it is unclear whether the quadratic speedup is sufficient to overcome the inherently slower quantum clock speeds and  overheads due to \hyperref[prim:FTQC]{fault-tolerant quantum computation} for practical instance sizes. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
\printbibliography[heading=secbib,segment=\therefsegment]

\end{refsection}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage 

\begin{refsection}

\subsection{Monte Carlo methods: Option pricing}\label{appl:OptionsPricing}

\subsubsection*{Overview}

Many financial instruments require an estimate of the average of some function of a stochastic variable within a window of time. To compute this average, one can use Monte Carlo methods to perform many simulations of the stochastic process over the time window, evaluate the function (which can potentially depend on the path taken by the stochastic variable during the entire window), and numerically estimate the average. 
While the setup and details of the problems may vary from one use case to another, the underlying methods are often quite similar. As an archetypal example of this problem, we will focus on the problem of pricing derivatives, such as options, but we remark that many of these results can be carried over to other use cases, such as computing Greeks, credit valuation adjustments, value at risk, etc.

Derivatives are financial instruments that, roughly speaking, allow the parties involved to benefit when an asset (such as a stock) increases or decreases in value, but without having to already hold the asset itself. One type of derivative---called an ``option''---is a contract that permits the holder to either purchase (``call option'') or sell (``put option'') an underlying asset at a fixed, predetermined price (the ``strike price'') at or prior to some predetermined time in the future (``the exercise window''). The seller of the option is obligated to either sell or buy the asset, should the holder choose to exercise the option.

How, then, should one decide on a price for the option (i.e., the amount the holder must pay for the contract, not the strike price)? The well-known Black--Scholes (or Black--Scholes--Merton) model provides one approach to pricing options, making a few assumptions about the underlying assets and the rules of the contract. More complicated options can be considered that include, for example, multiple assets  in the contract (e.g., basket options), multiple possible exercise windows (e.g., Bermudan or American options), etc.

Typically, options are priced by running Monte Carlo sampling on the value of the underlying asset(s) and determining the expected profit or loss from a given position, which can be translated into a price that the purchaser must pay. Options with a larger potential downside for the seller should cost a larger amount to purchase. For more information on options and Monte Carlo methods in the context of computational finance, see \cite{hull2017optionsTextbook,glasserman2004monte}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Actual end-to-end problem(s) solved}

Suppose you want to price an option based on an underlying asset. The price of the asset is a random variable $X$ that follows a known (or assumed) stochastic process that models the market market for the underlying asset. The option has a known payoff function $f(X)$ (e.g., the difference between the price of the asset at each time step minus the strike price over the trajectory, or zero, whichever is larger). For options that depend on more than one underlying asset or on asset prices at multiple distinct points in time, the random variable $X$ would represent a vector of data containing all information needed to compute the payoff.  Given these inputs, the end-to-end problem is to compute the an estimate of the expected payoff $\mathbb{E}_{X}(f(X))$ that lies within a certain error tolerance $\epsilon$ with high probability. This quantity is then used to determine a price to charge for the option.

Using the assumed stochastic model for the price of the asset, one can develop a stochastic differential equation for the average payoff of the option. In limited cases, one can compute the average payoff analytically, as in the case of the famous Black--Scholes formula for the price of European call options, for which the 1997 Nobel Prize in economics was awarded. The Black--Scholes differential equation for the price of an asset at time $t$ is given by
\begin{equation}
d X_t = X_t \alpha dt + X_t \sigma d W_t,
\end{equation}
where $X_t$ is the price of the underlying asset at time $t$, $\alpha$ is a parameter known as the ``drift'' of the asset, $\sigma$ is the volatility (the standard deviation of the underlying returns), and $d W_t$ is an increment of an accompanying Brownian motion $W_t$. Using It\^o's lemma, one can derive a differential equation for the payoff function of the option at time $t$ and, in limited cases (with several assumptions), one can solve the differential equation analytically. In practice, however, different types of contract have more complex definitions and fewer assumptions and, as a consequence, the differential equation cannot be solved analytically. Quantum approaches to numerically solving the stochastic differential equation have been proposed, including finite difference methods~\cite{miyamoto2021pricing}, Hamiltonian simulation~\cite{gonzalez2021simulating}, and quantum random walks~\cite{linden2022quantum}, etc. For more detail on quantum approaches to solving differential equations, see the \hyperref[appl:DiffEq]{differential equations} section of this document. In many real-world derivative pricing use cases, the underlying differential equation becomes intractable. Thus, the most common classical method of computing the average payoff of an option is not through solving the stochastic differential equation, but rather through Monte Carlo sampling the random process $X$ directly. To do so, one generates a large number of price trajectories over the chosen time range, and the average payoff is computed numerically. In what follows, we will focus on quantum approaches to Monte Carlo estimation, which was pioneered in~\cite{montanaro2015QMonteCarlo} and subsequently applied to several problems in finance (e.g., \cite{rebentrost2018MonteCarloDerivatives,stamatopoulos2020option,stamatopoulos2022towards,han2022quantum,woerner2019quantum}).
However, we remark that other approaches to solving this problem that do not make use of Monte Carlo methods have also been proposed (e.g., \cite{rebentrost2022QuantumFinanceMartingalePrediction}), and that this is an area of active research.


If a different financial instrument is desired, such as value at risk or credit valuation adjustments, the function to be computed may be quite different, but the approach is often the same: simulate the underlying stochastic evolution several times, and estimate the desired quantity numerically.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Dominant resource cost/complexity}

In~\cite{rebentrost2018MonteCarloDerivatives,stamatopoulos2020option}, the quantum speedup of Monte Carlo estimation from \cite{montanaro2015QMonteCarlo} is applied to solve the option pricing problem. We briefly explain the method and its dominant cost. First of all, this requires discretizing the set of values the random variable $X$ can take, which we index by the label $x$. Let $N$ denote the number of values and $n = \lceil \log_2(N) \rceil$ denote the number of qubits needed to hold the state $\ket{x}$. The first step is to load the probabilities for the future prices of the asset into the amplitudes of a quantum state, that is, the state
\begin{equation}
    \sum_x \sqrt{p_x} \ket{x}
\end{equation}
where $p_x$ is the probability that $x$ is observed in the corresponding classical Monte Carlo simulation. 

Second, a subroutine is employed that computes information about the payoff function into an ancilla register using coherent arithmetic. More precisely, the angle $\theta_x$ is computed (rounded to some finite number of bits of precision), where $\sin(\theta_x) = \sqrt{f(x)}$. (For simplicity, here we assume $0 \leq f(x) \leq 1$ for all $x$, but we revisit this point later.)
This yields 
\begin{equation}
    \sum_x \sqrt{p_x} \ket{x}\ket{\theta_x}\,.
\end{equation}
Third, the amplitude $\sqrt{f(x)}$ is loaded into the amplitude of an ancilla register by applying the map $\ket{\theta}\ket{0} \mapsto \ket{\theta}(\sin(\theta) \ket{0} + \cos(\theta)\ket{1})$. This gives
\begin{equation}\label{eq:option_pricing_state}
    \left(\sum_x \sqrt{p_xf(x)} \ket{x}\ket{\theta_x}\right)\ket{0} + \left(\sum_x \sqrt{p_x(1-f(x))} \ket{x}\ket{\theta_x}\right)\ket{1}\,.
\end{equation}
The probability of measuring the final ancilla in $\ket{0}$ is precisely $\mathbb{E}_X(f(X))$. Thus, the final step is to apply \hyperref[prim:AmpEst]{quantum amplitude estimation} (which requires many calls to the unitary that produces the state above) to obtain an estimate to error $\epsilon$. 

If $0 \leq f(x) \leq 1$ does not hold, the above approach needs to be modified for example by shifting and rescaling $f$ over a sequence of intervals of increasing length, as discussed in \cite{montanaro2015QMonteCarlo, rebentrost2018MonteCarloDerivatives}. To fit the range of $f$ into the interval $[0,1]$, we should expect the function $f$ will need to be scaled down by a factor on the order of the standard deviation $\sigma = \sqrt{\mathbb{E}_X(f(X)^2)-(\mathbb{E}f(x))^2}$. Thus, to achieve error $\epsilon$, QAE must be performed to precision $\epsilon/\sigma$ instead of $\epsilon$. 

There are three components to the algorithm that each contribute to the resource cost:
\begin{itemize}
    \item Loading the distribution with amplitudes $\sqrt{p_x}$. The gate complexity of this step is roughly the same as the time complexity of classically drawing a Monte Carlo sample, although for certain distributions it could be faster (e.g.~a quadratic quantum speedup can be obtained if $p_x$ is the stationary distribution of a Markov process \cite{szegedy2004QMarkovChainSearch}). Alternatively, if a functional form for $p_x$ is known, the methods of \cite{mcardle2022StatePreparation} could be used to approximately prepare the state.  Finally, \cite{stamatopoulos2020option} proposes using a quantum Generative Adversarial Network (qGAN), a \hyperref[prim:VQA]{variational} ansatz, which could reduce the resources but requires a training phase. 
    \item Coherent arithmetic to compute the rotation angle $\theta_x$. This depends on the complexity of the function $f$, but can generally be accomplished in comparable gate complexity as classical arithmetic, i.e.~$\mathrm{poly}(n)$. In \cite{stamatopoulos2023derivativeQSP}, it was shown how the payoff can instead be put directly into the amplitude, without ever computing $\theta_x$, using \hyperref[prim:QSP]{quantum signal processing methods} \cite{mcardle2022StatePreparation}. 
    \item \hyperref[prim:AmpEst]{Quantum amplitude estimation} to precision $\epsilon/\sigma$, which requires $\bigOt{\sigma/\epsilon}$ repetitions of the above two costs to achieve an $\epsilon$-estimate on the quantity $\mathbb{E}_X f(X)$.  
\end{itemize}

Overall, from \cite[Theorem 2.5]{montanaro2015QMonteCarlo} the complexity is 
\begin{equation}
    \frac{\sigma}{\epsilon}\log^{3/2}(\sigma/\epsilon)\log(\log(\sigma/\epsilon)) \cdot \mathrm{poly}(n)\,,
\end{equation}
with the $\mathrm{poly}(n)$ factor generally on the same order as the time required to draw and process a single classical Monte Carlo sample. 

One can also consider approaches to solving this problem that rely on quantum techniques for \hyperref[appl:DiffEq]{solving differential equations}, though we refer the reader to that section for more details.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Existing error corrected resource estimates}

Detailed resource estimations for benchmark option-pricing problems (known as autocallable and Target Accrual Redemption Forward, or TARF) were studied in~\cite{chakrabarti2021threshold}. The authors studied real-world use cases and problem sizes that are relevant to current financial institutions, but on the more challenging side for classical methods. For a basket autocallable with 3 underlying assets, 5 payment days, and a knock-in put option with 20 barrier dates, the authors found that one would need about $8000$ logical qubits, a $T$-depth of $5.4\times 10^7$, and a $T$-count of about $1.2\times 10^{10}$, using the most efficient methods they studied. For a TARF with 1 underlying and 26 payment dates, one needs about $1.2 \times 10^4$ logical qubits, a $T$-depth of about $8.2\times 10^7$, and a $T$-count of about $9.8\times 10^9$. A follow up analysis \cite{stamatopoulos2023derivativeQSP} involving a \hyperref[prim:QSP]{quantum signal processing} approach subsequently reduced these estimates to $4.7 \times 10^3$ logical qubits, $4.5 \times 10^7$ $T$-depth, and $2.4 \times 10^9$ $T$-count. For comparison, classical Monte Carlo methods are roughly estimated to require 1--10 seconds and $4 \times 10^4$ samples to achieve the same accuracy on these examples.

Similar analyses were performed in~\cite{stamatopoulos2022towards} for the computation of ``the Greeks'', which are quantities that measure the sensitivity of a derivative to various parameters. To compute the Greeks of an option, one needs to compute the derivative of the payoff function with respect to, for example, the price of the underlying. To to do this on a quantum computer, one needs to be able to estimate both the expectation of the payoff function, and have a way of computing gradients. The authors apply several quantum methods of computing gradients in order to calculate the Greeks, in addition to the quantum approaches to Monte Carlo methods used. Using a \hyperref[prim:GradientEstimation]{quantum gradient} method to compute Greeks of an option, the authors estimate that one would need about $1.2 \times 10^4$ logical qubits and a $T$-depth of around $10^8$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Caveats}

There are many types of options and derivatives that may not be accurately captured by these simple models. Some payoff functions are path-dependent, and hence one cannot simply use the asset value at some fixed time to compute the cost, but rather the cost depends on the trajectory the random variable takes in each Monte Carlo sample. 

Moreover, classical approaches to Monte Carlo sampling often allow for massive parallelization, as each simulation of the underlying asset can be done independently. By contrast, quantum algorithms for this problem require a \emph{serial} approach, as the subroutines in the quantum algorithm must be run one after another without measurement and restart if the quadratic advantage is to be realized. When the slower clock speeds found in quantum devices is also taken into account, the requirements for a quantum speedup over classical methods become more stringent, as much larger problem sizes are required to achieve practical advantage. For further reading, see~\cite[Sec. 2.3]{bouland2020prospects}, for example.

It is worth noting that in certain cases the number of classical samples needed to achieve error $\epsilon$ can be reduced from the naive $\bigO{\sigma^2/\epsilon^2}$, cutting into the quadratic quantum speedup. In particular, quasi--Monte Carlo methods, which sample possible trajectories of the underlying assets nonrandomly can achieve a nearly quadratic speedup compared to traditional classical Monte Carlo methods, but gain an exponential dependence on the number of underlying assets (``curse of dimensionality'') see \cite[Chapter 5]{glasserman2004monte}, which limits their use. The number of samples can also potentially be reduced classically via multilevel Monte Carlo methods \cite{giles2015multilevelMC}. However, when and how these methods work is delicate and must be evaluated on a case-by-case basis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Comparable classical complexity and challenging instance sizes}

Classical approaches to option pricing comprise some of the largest computational costs incurred by financial institutions.
In the traditional approach to solving the option pricing problem, Monte Carlo sampling is required to simulate the evolution of the underlying asset over the time horizon of the option, and it can be slow to converge.
In particular, denote the expectation value of $f(X)$ by $V:=\mathbb{E}_{X}(f(X))$, and the variance of $f(X)$ by $\sigma^2$. Classical Monte Carlo methods computes an estimate $\hat{V}$ for $V$ formed by averaging $f(X)$ for $M$ independent samples of $X$. By Chebyshev's inequality,
\begin{equation*}
    \mathrm{Pr}(|V-\hat{V}|\geq \epsilon)\leq \frac{\sigma^2}{M\epsilon^2}.
\end{equation*}
Thus, classically one needs $M\sim\mathcal{O}(\sigma^2/\epsilon^2)$ samples to find an estimate $\hat{V}$ within a 99\% confidence interval \cite{montanaro2015QMonteCarlo}.

In typical industrial scenarios, options can be priced to sufficient operational precision after roughly a few seconds of runtime, sampling as many as tens of thousands of Monte Carlo trajectories.

Alternatively, a tensor-network-based classical approach to option pricing was proposed by \cite{kastoryano2022highly} that could lead to significant advantages over traditional classical methods in some cases. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Speedup}
The classical algorithm requires $M = \bigO{\sigma^2/\epsilon^2}$ samples whereas the quantum algorithm requires only $\bigOt{\sqrt{M}} = \bigOt{\sigma/\epsilon}$ samples. The gate cost of a sample is roughly the same classically and quantumly, and thus the speedup is (nearly) quadratic, inherited from the quadratic speedup of \hyperref[prim:AmpEst]{QAE}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Outlook}

In~\cite{chakrabarti2021threshold}, the authors place an upper bound on the resources required for pricing options on quantum computers, and they provide a goalpost for quantum hardware development to be able to outperform classical Monte Carlo methods. In particular, the authors estimate that a quantum device would need to be able to execute about $10^7$ layers of $T$-gates per second. Moreover, the code distance for \hyperref[prim:FTQC]{fault-tolerant} implementation would need to be chosen large enough to support $10^{10}$ total error-free logical operations. These requirements translate to a logical clock rate of about $50$MHz that would be needed in order to compete with current classical Monte Carlo methods. This clock speed is orders of magnitude faster than what is foreseeably possible given the current status of physical hardware and currently known methods for \hyperref[prim:LatticeSurgery]{performing logical gates in the surface code}.

While the resource requirements for pricing of derivatives are quite stringent, this is nevertheless an area of active research. For example, a new ``analog'' quantum representation of stochastic processes was developed in \cite{bouland2023quantum} that can compute $\epsilon$-accurate estimates of time averages (over $T$ timesteps) of certain functions of stochastic processes in time $O(\mathrm{polylog}(T)\epsilon^{-c})$, where $3/2 < c <2$, an exponential speedup over classical methods in the parameter $T$. The analog nature of their method leads to additional caveats, and finding concrete applications of this method remains an interesting open question. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
\printbibliography[heading=secbib,segment=\therefsegment]

\end{refsection}

